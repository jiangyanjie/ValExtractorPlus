{
    "id": 262,
    "expression": "memory",
    "projectName": "tinkerpop",
    "commitID": "cf2e3b1a99dee9e45540859202cf784913658e47",
    "filePath": "spark-gremlin/src/main/java/org/apache/tinkerpop/gremlin/spark/process/computer/SparkGraphComputer.java",
    "occurrences": 10,
    "isArithmeticExpression": 1,
    "isGetTypeMethod": 1,
    "expressionList": [
        {
            "nodeContext": "memory",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 6,
                "startLineNumber": 178,
                "startColumnNumber": 45,
                "endLineNumber": 178,
                "endColumnNumber": 51
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 32,
                        "startLineNumber": 178,
                        "startColumnNumber": 20,
                        "endLineNumber": 178,
                        "endColumnNumber": 52
                    },
                    "nodeContext": "this.vertexProgram.setup(memory)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 6,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 33,
                        "startLineNumber": 178,
                        "startColumnNumber": 20,
                        "endLineNumber": 178,
                        "endColumnNumber": 53
                    },
                    "nodeContext": "this.vertexProgram.setup(memory);\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 7,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 2548,
                        "startLineNumber": 175,
                        "startColumnNumber": 48,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 216,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 2580,
                        "startLineNumber": 175,
                        "startColumnNumber": 16,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (null != this.vertexProgram) {\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 222,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                "nodePosition": {
                    "charLength": 33,
                    "startLineNumber": 178,
                    "startColumnNumber": 20,
                    "endLineNumber": 178,
                    "endColumnNumber": 53
                },
                "nodeContext": "this.vertexProgram.setup(memory);\n",
                "nodeType": "ExpressionStatement",
                "astNodeNumber": 7,
                "astHeight": 4
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.spark.process.computer.SparkMemory"
        },
        {
            "nodeContext": "memory",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 6,
                "startLineNumber": 179,
                "startColumnNumber": 20,
                "endLineNumber": 179,
                "endColumnNumber": 26
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 36,
                        "startLineNumber": 179,
                        "startColumnNumber": 20,
                        "endLineNumber": 179,
                        "endColumnNumber": 56
                    },
                    "nodeContext": "memory.broadcastMemory(sparkContext)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 4,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 37,
                        "startLineNumber": 179,
                        "startColumnNumber": 20,
                        "endLineNumber": 179,
                        "endColumnNumber": 57
                    },
                    "nodeContext": "memory.broadcastMemory(sparkContext);\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 5,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 2548,
                        "startLineNumber": 175,
                        "startColumnNumber": 48,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 216,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 2580,
                        "startLineNumber": 175,
                        "startColumnNumber": 16,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (null != this.vertexProgram) {\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 222,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                "nodePosition": {
                    "charLength": 37,
                    "startLineNumber": 179,
                    "startColumnNumber": 20,
                    "endLineNumber": 179,
                    "endColumnNumber": 57
                },
                "nodeContext": "memory.broadcastMemory(sparkContext);\n",
                "nodeType": "ExpressionStatement",
                "astNodeNumber": 5,
                "astHeight": 3
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.spark.process.computer.SparkMemory"
        },
        {
            "nodeContext": "memory",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 6,
                "startLineNumber": 186,
                "startColumnNumber": 24,
                "endLineNumber": 186,
                "endColumnNumber": 30
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 22,
                        "startLineNumber": 186,
                        "startColumnNumber": 24,
                        "endLineNumber": 186,
                        "endColumnNumber": 46
                    },
                    "nodeContext": "memory.setInTask(true)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 4,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 23,
                        "startLineNumber": 186,
                        "startColumnNumber": 24,
                        "endLineNumber": 186,
                        "endColumnNumber": 47
                    },
                    "nodeContext": "memory.setInTask(true);\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 5,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.WhileStatement,body]",
                    "nodePosition": {
                        "charLength": 546,
                        "startLineNumber": 185,
                        "startColumnNumber": 33,
                        "endLineNumber": 195,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "{\n  memory.setInTask(true);\n  viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n  memory.setInTask(false);\n  if (this.vertexProgram.terminate(memory))   break;\n else {\n    memory.incrIteration();\n    memory.broadcastMemory(sparkContext);\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 39,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 559,
                        "startLineNumber": 185,
                        "startColumnNumber": 20,
                        "endLineNumber": 195,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "while (true) {\n  memory.setInTask(true);\n  viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n  memory.setInTask(false);\n  if (this.vertexProgram.terminate(memory))   break;\n else {\n    memory.incrIteration();\n    memory.broadcastMemory(sparkContext);\n  }\n}\n",
                    "nodeType": "WhileStatement",
                    "astNodeNumber": 41,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 2548,
                        "startLineNumber": 175,
                        "startColumnNumber": 48,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 216,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 2580,
                        "startLineNumber": 175,
                        "startColumnNumber": 16,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (null != this.vertexProgram) {\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 222,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                "nodePosition": {
                    "charLength": 23,
                    "startLineNumber": 186,
                    "startColumnNumber": 24,
                    "endLineNumber": 186,
                    "endColumnNumber": 47
                },
                "nodeContext": "memory.setInTask(true);\n",
                "nodeType": "ExpressionStatement",
                "astNodeNumber": 5,
                "astHeight": 3
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.spark.process.computer.SparkMemory"
        },
        {
            "nodeContext": "memory",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 6,
                "startLineNumber": 187,
                "startColumnNumber": 113,
                "endLineNumber": 187,
                "endColumnNumber": 119
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.Assignment,rightHandSide]",
                    "nodePosition": {
                        "charLength": 106,
                        "startLineNumber": 187,
                        "startColumnNumber": 42,
                        "endLineNumber": 187,
                        "endColumnNumber": 148
                    },
                    "nodeContext": "SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 7,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 124,
                        "startLineNumber": 187,
                        "startColumnNumber": 24,
                        "endLineNumber": 187,
                        "endColumnNumber": 148
                    },
                    "nodeContext": "viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration)",
                    "nodeType": "Assignment",
                    "astNodeNumber": 9,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 125,
                        "startLineNumber": 187,
                        "startColumnNumber": 24,
                        "endLineNumber": 187,
                        "endColumnNumber": 149
                    },
                    "nodeContext": "viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 10,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.WhileStatement,body]",
                    "nodePosition": {
                        "charLength": 546,
                        "startLineNumber": 185,
                        "startColumnNumber": 33,
                        "endLineNumber": 195,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "{\n  memory.setInTask(true);\n  viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n  memory.setInTask(false);\n  if (this.vertexProgram.terminate(memory))   break;\n else {\n    memory.incrIteration();\n    memory.broadcastMemory(sparkContext);\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 39,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 559,
                        "startLineNumber": 185,
                        "startColumnNumber": 20,
                        "endLineNumber": 195,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "while (true) {\n  memory.setInTask(true);\n  viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n  memory.setInTask(false);\n  if (this.vertexProgram.terminate(memory))   break;\n else {\n    memory.incrIteration();\n    memory.broadcastMemory(sparkContext);\n  }\n}\n",
                    "nodeType": "WhileStatement",
                    "astNodeNumber": 41,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 2548,
                        "startLineNumber": 175,
                        "startColumnNumber": 48,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 216,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 2580,
                        "startLineNumber": 175,
                        "startColumnNumber": 16,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (null != this.vertexProgram) {\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 222,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                "nodePosition": {
                    "charLength": 125,
                    "startLineNumber": 187,
                    "startColumnNumber": 24,
                    "endLineNumber": 187,
                    "endColumnNumber": 149
                },
                "nodeContext": "viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n",
                "nodeType": "ExpressionStatement",
                "astNodeNumber": 10,
                "astHeight": 4
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.spark.process.computer.SparkMemory"
        },
        {
            "nodeContext": "memory",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 6,
                "startLineNumber": 188,
                "startColumnNumber": 24,
                "endLineNumber": 188,
                "endColumnNumber": 30
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 23,
                        "startLineNumber": 188,
                        "startColumnNumber": 24,
                        "endLineNumber": 188,
                        "endColumnNumber": 47
                    },
                    "nodeContext": "memory.setInTask(false)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 4,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 24,
                        "startLineNumber": 188,
                        "startColumnNumber": 24,
                        "endLineNumber": 188,
                        "endColumnNumber": 48
                    },
                    "nodeContext": "memory.setInTask(false);\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 5,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.WhileStatement,body]",
                    "nodePosition": {
                        "charLength": 546,
                        "startLineNumber": 185,
                        "startColumnNumber": 33,
                        "endLineNumber": 195,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "{\n  memory.setInTask(true);\n  viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n  memory.setInTask(false);\n  if (this.vertexProgram.terminate(memory))   break;\n else {\n    memory.incrIteration();\n    memory.broadcastMemory(sparkContext);\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 39,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 559,
                        "startLineNumber": 185,
                        "startColumnNumber": 20,
                        "endLineNumber": 195,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "while (true) {\n  memory.setInTask(true);\n  viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n  memory.setInTask(false);\n  if (this.vertexProgram.terminate(memory))   break;\n else {\n    memory.incrIteration();\n    memory.broadcastMemory(sparkContext);\n  }\n}\n",
                    "nodeType": "WhileStatement",
                    "astNodeNumber": 41,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 2548,
                        "startLineNumber": 175,
                        "startColumnNumber": 48,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 216,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 2580,
                        "startLineNumber": 175,
                        "startColumnNumber": 16,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (null != this.vertexProgram) {\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 222,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                "nodePosition": {
                    "charLength": 24,
                    "startLineNumber": 188,
                    "startColumnNumber": 24,
                    "endLineNumber": 188,
                    "endColumnNumber": 48
                },
                "nodeContext": "memory.setInTask(false);\n",
                "nodeType": "ExpressionStatement",
                "astNodeNumber": 5,
                "astHeight": 3
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.spark.process.computer.SparkMemory"
        },
        {
            "nodeContext": "memory",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 6,
                "startLineNumber": 189,
                "startColumnNumber": 57,
                "endLineNumber": 189,
                "endColumnNumber": 63
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                    "nodePosition": {
                        "charLength": 36,
                        "startLineNumber": 189,
                        "startColumnNumber": 28,
                        "endLineNumber": 189,
                        "endColumnNumber": 64
                    },
                    "nodeContext": "this.vertexProgram.terminate(memory)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 6,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 251,
                        "startLineNumber": 189,
                        "startColumnNumber": 24,
                        "endLineNumber": 194,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "if (this.vertexProgram.terminate(memory)) break;\n else {\n  memory.incrIteration();\n  memory.broadcastMemory(sparkContext);\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 18,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.WhileStatement,body]",
                    "nodePosition": {
                        "charLength": 546,
                        "startLineNumber": 185,
                        "startColumnNumber": 33,
                        "endLineNumber": 195,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "{\n  memory.setInTask(true);\n  viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n  memory.setInTask(false);\n  if (this.vertexProgram.terminate(memory))   break;\n else {\n    memory.incrIteration();\n    memory.broadcastMemory(sparkContext);\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 39,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 559,
                        "startLineNumber": 185,
                        "startColumnNumber": 20,
                        "endLineNumber": 195,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "while (true) {\n  memory.setInTask(true);\n  viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n  memory.setInTask(false);\n  if (this.vertexProgram.terminate(memory))   break;\n else {\n    memory.incrIteration();\n    memory.broadcastMemory(sparkContext);\n  }\n}\n",
                    "nodeType": "WhileStatement",
                    "astNodeNumber": 41,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 2548,
                        "startLineNumber": 175,
                        "startColumnNumber": 48,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 216,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 2580,
                        "startLineNumber": 175,
                        "startColumnNumber": 16,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (null != this.vertexProgram) {\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 222,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                "nodePosition": {
                    "charLength": 36,
                    "startLineNumber": 189,
                    "startColumnNumber": 28,
                    "endLineNumber": 189,
                    "endColumnNumber": 64
                },
                "nodeContext": "this.vertexProgram.terminate(memory)",
                "nodeType": "MethodInvocation",
                "astNodeNumber": 6,
                "astHeight": 3
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.spark.process.computer.SparkMemory"
        },
        {
            "nodeContext": "memory",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 6,
                "startLineNumber": 192,
                "startColumnNumber": 28,
                "endLineNumber": 192,
                "endColumnNumber": 34
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 22,
                        "startLineNumber": 192,
                        "startColumnNumber": 28,
                        "endLineNumber": 192,
                        "endColumnNumber": 50
                    },
                    "nodeContext": "memory.incrIteration()",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 23,
                        "startLineNumber": 192,
                        "startColumnNumber": 28,
                        "endLineNumber": 192,
                        "endColumnNumber": 51
                    },
                    "nodeContext": "memory.incrIteration();\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 4,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,elseStatement]",
                    "nodePosition": {
                        "charLength": 145,
                        "startLineNumber": 191,
                        "startColumnNumber": 29,
                        "endLineNumber": 194,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "{\n  memory.incrIteration();\n  memory.broadcastMemory(sparkContext);\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 10,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 251,
                        "startLineNumber": 189,
                        "startColumnNumber": 24,
                        "endLineNumber": 194,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "if (this.vertexProgram.terminate(memory)) break;\n else {\n  memory.incrIteration();\n  memory.broadcastMemory(sparkContext);\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 18,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.WhileStatement,body]",
                    "nodePosition": {
                        "charLength": 546,
                        "startLineNumber": 185,
                        "startColumnNumber": 33,
                        "endLineNumber": 195,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "{\n  memory.setInTask(true);\n  viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n  memory.setInTask(false);\n  if (this.vertexProgram.terminate(memory))   break;\n else {\n    memory.incrIteration();\n    memory.broadcastMemory(sparkContext);\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 39,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 559,
                        "startLineNumber": 185,
                        "startColumnNumber": 20,
                        "endLineNumber": 195,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "while (true) {\n  memory.setInTask(true);\n  viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n  memory.setInTask(false);\n  if (this.vertexProgram.terminate(memory))   break;\n else {\n    memory.incrIteration();\n    memory.broadcastMemory(sparkContext);\n  }\n}\n",
                    "nodeType": "WhileStatement",
                    "astNodeNumber": 41,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 2548,
                        "startLineNumber": 175,
                        "startColumnNumber": 48,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 216,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 2580,
                        "startLineNumber": 175,
                        "startColumnNumber": 16,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (null != this.vertexProgram) {\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 222,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                "nodePosition": {
                    "charLength": 23,
                    "startLineNumber": 192,
                    "startColumnNumber": 28,
                    "endLineNumber": 192,
                    "endColumnNumber": 51
                },
                "nodeContext": "memory.incrIteration();\n",
                "nodeType": "ExpressionStatement",
                "astNodeNumber": 4,
                "astHeight": 3
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.spark.process.computer.SparkMemory"
        },
        {
            "nodeContext": "memory",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 6,
                "startLineNumber": 193,
                "startColumnNumber": 28,
                "endLineNumber": 193,
                "endColumnNumber": 34
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 36,
                        "startLineNumber": 193,
                        "startColumnNumber": 28,
                        "endLineNumber": 193,
                        "endColumnNumber": 64
                    },
                    "nodeContext": "memory.broadcastMemory(sparkContext)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 4,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 37,
                        "startLineNumber": 193,
                        "startColumnNumber": 28,
                        "endLineNumber": 193,
                        "endColumnNumber": 65
                    },
                    "nodeContext": "memory.broadcastMemory(sparkContext);\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 5,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,elseStatement]",
                    "nodePosition": {
                        "charLength": 145,
                        "startLineNumber": 191,
                        "startColumnNumber": 29,
                        "endLineNumber": 194,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "{\n  memory.incrIteration();\n  memory.broadcastMemory(sparkContext);\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 10,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 251,
                        "startLineNumber": 189,
                        "startColumnNumber": 24,
                        "endLineNumber": 194,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "if (this.vertexProgram.terminate(memory)) break;\n else {\n  memory.incrIteration();\n  memory.broadcastMemory(sparkContext);\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 18,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.WhileStatement,body]",
                    "nodePosition": {
                        "charLength": 546,
                        "startLineNumber": 185,
                        "startColumnNumber": 33,
                        "endLineNumber": 195,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "{\n  memory.setInTask(true);\n  viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n  memory.setInTask(false);\n  if (this.vertexProgram.terminate(memory))   break;\n else {\n    memory.incrIteration();\n    memory.broadcastMemory(sparkContext);\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 39,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 559,
                        "startLineNumber": 185,
                        "startColumnNumber": 20,
                        "endLineNumber": 195,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "while (true) {\n  memory.setInTask(true);\n  viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n  memory.setInTask(false);\n  if (this.vertexProgram.terminate(memory))   break;\n else {\n    memory.incrIteration();\n    memory.broadcastMemory(sparkContext);\n  }\n}\n",
                    "nodeType": "WhileStatement",
                    "astNodeNumber": 41,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 2548,
                        "startLineNumber": 175,
                        "startColumnNumber": 48,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 216,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 2580,
                        "startLineNumber": 175,
                        "startColumnNumber": 16,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (null != this.vertexProgram) {\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 222,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                "nodePosition": {
                    "charLength": 37,
                    "startLineNumber": 193,
                    "startColumnNumber": 28,
                    "endLineNumber": 193,
                    "endColumnNumber": 65
                },
                "nodeContext": "memory.broadcastMemory(sparkContext);\n",
                "nodeType": "ExpressionStatement",
                "astNodeNumber": 5,
                "astHeight": 3
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.spark.process.computer.SparkMemory"
        },
        {
            "nodeContext": "memory",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 6,
                "startLineNumber": 212,
                "startColumnNumber": 57,
                "endLineNumber": 212,
                "endColumnNumber": 63
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ConditionalExpression,expression]",
                    "nodePosition": {
                        "charLength": 14,
                        "startLineNumber": 212,
                        "startColumnNumber": 49,
                        "endLineNumber": 212,
                        "endColumnNumber": 63
                    },
                    "nodeContext": "null == memory",
                    "nodeType": "InfixExpression",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.VariableDeclarationFragment,initializer]",
                    "nodePosition": {
                        "charLength": 56,
                        "startLineNumber": 212,
                        "startColumnNumber": 49,
                        "endLineNumber": 212,
                        "endColumnNumber": 105
                    },
                    "nodeContext": "null == memory ? new MapMemory() : new MapMemory(memory)",
                    "nodeType": "ConditionalExpression",
                    "astNodeNumber": 11,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.VariableDeclarationStatement,fragments]",
                    "nodePosition": {
                        "charLength": 70,
                        "startLineNumber": 212,
                        "startColumnNumber": 35,
                        "endLineNumber": 212,
                        "endColumnNumber": 105
                    },
                    "nodeContext": "finalMemory=null == memory ? new MapMemory() : new MapMemory(memory)",
                    "nodeType": "VariableDeclarationFragment",
                    "astNodeNumber": 13,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 90,
                        "startLineNumber": 212,
                        "startColumnNumber": 16,
                        "endLineNumber": 212,
                        "endColumnNumber": 106
                    },
                    "nodeContext": "final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n",
                    "nodeType": "VariableDeclarationStatement",
                    "astNodeNumber": 19,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                "nodePosition": {
                    "charLength": 90,
                    "startLineNumber": 212,
                    "startColumnNumber": 16,
                    "endLineNumber": 212,
                    "endColumnNumber": 106
                },
                "nodeContext": "final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n",
                "nodeType": "VariableDeclarationStatement",
                "astNodeNumber": 19,
                "astHeight": 6
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.spark.process.computer.SparkMemory"
        },
        {
            "nodeContext": "memory",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 6,
                "startLineNumber": 212,
                "startColumnNumber": 98,
                "endLineNumber": 212,
                "endColumnNumber": 104
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ConditionalExpression,elseExpression]",
                    "nodePosition": {
                        "charLength": 21,
                        "startLineNumber": 212,
                        "startColumnNumber": 84,
                        "endLineNumber": 212,
                        "endColumnNumber": 105
                    },
                    "nodeContext": "new MapMemory(memory)",
                    "nodeType": "ClassInstanceCreation",
                    "astNodeNumber": 4,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.VariableDeclarationFragment,initializer]",
                    "nodePosition": {
                        "charLength": 56,
                        "startLineNumber": 212,
                        "startColumnNumber": 49,
                        "endLineNumber": 212,
                        "endColumnNumber": 105
                    },
                    "nodeContext": "null == memory ? new MapMemory() : new MapMemory(memory)",
                    "nodeType": "ConditionalExpression",
                    "astNodeNumber": 11,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.VariableDeclarationStatement,fragments]",
                    "nodePosition": {
                        "charLength": 70,
                        "startLineNumber": 212,
                        "startColumnNumber": 35,
                        "endLineNumber": 212,
                        "endColumnNumber": 105
                    },
                    "nodeContext": "finalMemory=null == memory ? new MapMemory() : new MapMemory(memory)",
                    "nodeType": "VariableDeclarationFragment",
                    "astNodeNumber": 13,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 90,
                        "startLineNumber": 212,
                        "startColumnNumber": 16,
                        "endLineNumber": 212,
                        "endColumnNumber": 106
                    },
                    "nodeContext": "final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n",
                    "nodeType": "VariableDeclarationStatement",
                    "astNodeNumber": 19,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                "nodePosition": {
                    "charLength": 90,
                    "startLineNumber": 212,
                    "startColumnNumber": 16,
                    "endLineNumber": 212,
                    "endColumnNumber": 106
                },
                "nodeContext": "final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n",
                "nodeType": "VariableDeclarationStatement",
                "astNodeNumber": 19,
                "astHeight": 6
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.spark.process.computer.SparkMemory"
        }
    ],
    "positionList": [
        {
            "charLength": 6,
            "startLineNumber": 178,
            "startColumnNumber": 45,
            "endLineNumber": 178,
            "endColumnNumber": 51
        },
        {
            "charLength": 6,
            "startLineNumber": 179,
            "startColumnNumber": 20,
            "endLineNumber": 179,
            "endColumnNumber": 26
        },
        {
            "charLength": 6,
            "startLineNumber": 186,
            "startColumnNumber": 24,
            "endLineNumber": 186,
            "endColumnNumber": 30
        },
        {
            "charLength": 6,
            "startLineNumber": 187,
            "startColumnNumber": 113,
            "endLineNumber": 187,
            "endColumnNumber": 119
        },
        {
            "charLength": 6,
            "startLineNumber": 188,
            "startColumnNumber": 24,
            "endLineNumber": 188,
            "endColumnNumber": 30
        },
        {
            "charLength": 6,
            "startLineNumber": 189,
            "startColumnNumber": 57,
            "endLineNumber": 189,
            "endColumnNumber": 63
        },
        {
            "charLength": 6,
            "startLineNumber": 192,
            "startColumnNumber": 28,
            "endLineNumber": 192,
            "endColumnNumber": 34
        },
        {
            "charLength": 6,
            "startLineNumber": 193,
            "startColumnNumber": 28,
            "endLineNumber": 193,
            "endColumnNumber": 34
        },
        {
            "charLength": 6,
            "startLineNumber": 212,
            "startColumnNumber": 57,
            "endLineNumber": 212,
            "endColumnNumber": 63
        },
        {
            "charLength": 6,
            "startLineNumber": 212,
            "startColumnNumber": 98,
            "endLineNumber": 212,
            "endColumnNumber": 104
        }
    ],
    "layoutRelationDataList": [
        {
            "firstKey": 0,
            "secondKey": 1,
            "layout": 2
        },
        {
            "firstKey": 0,
            "secondKey": 2,
            "layout": 2
        },
        {
            "firstKey": 0,
            "secondKey": 3,
            "layout": 2
        },
        {
            "firstKey": 0,
            "secondKey": 4,
            "layout": 2
        },
        {
            "firstKey": 0,
            "secondKey": 5,
            "layout": 2
        },
        {
            "firstKey": 0,
            "secondKey": 6,
            "layout": 2
        },
        {
            "firstKey": 0,
            "secondKey": 7,
            "layout": 2
        },
        {
            "firstKey": 0,
            "secondKey": 8,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 9,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 0,
            "layout": 2
        },
        {
            "firstKey": 1,
            "secondKey": 2,
            "layout": 2
        },
        {
            "firstKey": 1,
            "secondKey": 3,
            "layout": 2
        },
        {
            "firstKey": 1,
            "secondKey": 4,
            "layout": 2
        },
        {
            "firstKey": 1,
            "secondKey": 5,
            "layout": 2
        },
        {
            "firstKey": 1,
            "secondKey": 6,
            "layout": 2
        },
        {
            "firstKey": 1,
            "secondKey": 7,
            "layout": 2
        },
        {
            "firstKey": 1,
            "secondKey": 8,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 9,
            "layout": 4
        },
        {
            "firstKey": 2,
            "secondKey": 0,
            "layout": 4
        },
        {
            "firstKey": 2,
            "secondKey": 1,
            "layout": 4
        },
        {
            "firstKey": 2,
            "secondKey": 3,
            "layout": 2
        },
        {
            "firstKey": 2,
            "secondKey": 4,
            "layout": 2
        },
        {
            "firstKey": 2,
            "secondKey": 5,
            "layout": 2
        },
        {
            "firstKey": 2,
            "secondKey": 6,
            "layout": 2
        },
        {
            "firstKey": 2,
            "secondKey": 7,
            "layout": 2
        },
        {
            "firstKey": 2,
            "secondKey": 8,
            "layout": 6
        },
        {
            "firstKey": 2,
            "secondKey": 9,
            "layout": 6
        },
        {
            "firstKey": 3,
            "secondKey": 0,
            "layout": 5
        },
        {
            "firstKey": 3,
            "secondKey": 1,
            "layout": 5
        },
        {
            "firstKey": 3,
            "secondKey": 2,
            "layout": 3
        },
        {
            "firstKey": 3,
            "secondKey": 4,
            "layout": 3
        },
        {
            "firstKey": 3,
            "secondKey": 5,
            "layout": 3
        },
        {
            "firstKey": 3,
            "secondKey": 6,
            "layout": 3
        },
        {
            "firstKey": 3,
            "secondKey": 7,
            "layout": 3
        },
        {
            "firstKey": 3,
            "secondKey": 8,
            "layout": 7
        },
        {
            "firstKey": 3,
            "secondKey": 9,
            "layout": 7
        },
        {
            "firstKey": 4,
            "secondKey": 0,
            "layout": 4
        },
        {
            "firstKey": 4,
            "secondKey": 1,
            "layout": 4
        },
        {
            "firstKey": 4,
            "secondKey": 2,
            "layout": 2
        },
        {
            "firstKey": 4,
            "secondKey": 3,
            "layout": 2
        },
        {
            "firstKey": 4,
            "secondKey": 5,
            "layout": 2
        },
        {
            "firstKey": 4,
            "secondKey": 6,
            "layout": 2
        },
        {
            "firstKey": 4,
            "secondKey": 7,
            "layout": 2
        },
        {
            "firstKey": 4,
            "secondKey": 8,
            "layout": 6
        },
        {
            "firstKey": 4,
            "secondKey": 9,
            "layout": 6
        },
        {
            "firstKey": 5,
            "secondKey": 0,
            "layout": 4
        },
        {
            "firstKey": 5,
            "secondKey": 1,
            "layout": 4
        },
        {
            "firstKey": 5,
            "secondKey": 2,
            "layout": 2
        },
        {
            "firstKey": 5,
            "secondKey": 3,
            "layout": 2
        },
        {
            "firstKey": 5,
            "secondKey": 4,
            "layout": 2
        },
        {
            "firstKey": 5,
            "secondKey": 6,
            "layout": 1
        },
        {
            "firstKey": 5,
            "secondKey": 7,
            "layout": 1
        },
        {
            "firstKey": 5,
            "secondKey": 8,
            "layout": 6
        },
        {
            "firstKey": 5,
            "secondKey": 9,
            "layout": 6
        },
        {
            "firstKey": 6,
            "secondKey": 0,
            "layout": 6
        },
        {
            "firstKey": 6,
            "secondKey": 1,
            "layout": 6
        },
        {
            "firstKey": 6,
            "secondKey": 2,
            "layout": 4
        },
        {
            "firstKey": 6,
            "secondKey": 3,
            "layout": 4
        },
        {
            "firstKey": 6,
            "secondKey": 4,
            "layout": 4
        },
        {
            "firstKey": 6,
            "secondKey": 5,
            "layout": 3
        },
        {
            "firstKey": 6,
            "secondKey": 7,
            "layout": 2
        },
        {
            "firstKey": 6,
            "secondKey": 8,
            "layout": 8
        },
        {
            "firstKey": 6,
            "secondKey": 9,
            "layout": 8
        },
        {
            "firstKey": 7,
            "secondKey": 0,
            "layout": 6
        },
        {
            "firstKey": 7,
            "secondKey": 1,
            "layout": 6
        },
        {
            "firstKey": 7,
            "secondKey": 2,
            "layout": 4
        },
        {
            "firstKey": 7,
            "secondKey": 3,
            "layout": 4
        },
        {
            "firstKey": 7,
            "secondKey": 4,
            "layout": 4
        },
        {
            "firstKey": 7,
            "secondKey": 5,
            "layout": 3
        },
        {
            "firstKey": 7,
            "secondKey": 6,
            "layout": 2
        },
        {
            "firstKey": 7,
            "secondKey": 8,
            "layout": 8
        },
        {
            "firstKey": 7,
            "secondKey": 9,
            "layout": 8
        },
        {
            "firstKey": 8,
            "secondKey": 0,
            "layout": 4
        },
        {
            "firstKey": 8,
            "secondKey": 1,
            "layout": 4
        },
        {
            "firstKey": 8,
            "secondKey": 2,
            "layout": 4
        },
        {
            "firstKey": 8,
            "secondKey": 3,
            "layout": 4
        },
        {
            "firstKey": 8,
            "secondKey": 4,
            "layout": 4
        },
        {
            "firstKey": 8,
            "secondKey": 5,
            "layout": 4
        },
        {
            "firstKey": 8,
            "secondKey": 6,
            "layout": 4
        },
        {
            "firstKey": 8,
            "secondKey": 7,
            "layout": 4
        },
        {
            "firstKey": 8,
            "secondKey": 9,
            "layout": 1
        },
        {
            "firstKey": 9,
            "secondKey": 0,
            "layout": 4
        },
        {
            "firstKey": 9,
            "secondKey": 1,
            "layout": 4
        },
        {
            "firstKey": 9,
            "secondKey": 2,
            "layout": 4
        },
        {
            "firstKey": 9,
            "secondKey": 3,
            "layout": 4
        },
        {
            "firstKey": 9,
            "secondKey": 4,
            "layout": 4
        },
        {
            "firstKey": 9,
            "secondKey": 5,
            "layout": 4
        },
        {
            "firstKey": 9,
            "secondKey": 6,
            "layout": 4
        },
        {
            "firstKey": 9,
            "secondKey": 7,
            "layout": 4
        },
        {
            "firstKey": 9,
            "secondKey": 8,
            "layout": 1
        }
    ]
}