{
    "id": 267,
    "expression": "this.workers",
    "projectName": "tinkerpop",
    "commitID": "cf2e3b1a99dee9e45540859202cf784913658e47",
    "filePath": "spark-gremlin/src/main/java/org/apache/tinkerpop/gremlin/spark/process/computer/SparkGraphComputer.java",
    "occurrences": 2,
    "isArithmeticExpression": 1,
    "isGetTypeMethod": 1,
    "expressionList": [
        {
            "nodeContext": "this.workers",
            "nodeType": "FieldAccess",
            "nodePosition": {
                "charLength": 12,
                "startLineNumber": 162,
                "startColumnNumber": 74,
                "endLineNumber": 162,
                "endColumnNumber": 86
            },
            "astNodeNumber": 3,
            "astHeight": 2,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.InfixExpression,rightOperand]",
                    "nodePosition": {
                        "charLength": 43,
                        "startLineNumber": 162,
                        "startColumnNumber": 43,
                        "endLineNumber": 162,
                        "endColumnNumber": 86
                    },
                    "nodeContext": "graphRDD.partitions().size() > this.workers",
                    "nodeType": "InfixExpression",
                    "astNodeNumber": 9,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                    "nodePosition": {
                        "charLength": 62,
                        "startLineNumber": 162,
                        "startColumnNumber": 24,
                        "endLineNumber": 162,
                        "endColumnNumber": 86
                    },
                    "nodeContext": "this.workersSet && graphRDD.partitions().size() > this.workers",
                    "nodeType": "InfixExpression",
                    "astNodeNumber": 13,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 207,
                        "startLineNumber": 162,
                        "startColumnNumber": 20,
                        "endLineNumber": 163,
                        "endColumnNumber": 67
                    },
                    "nodeContext": "if (this.workersSet && graphRDD.partitions().size() > this.workers) graphRDD=graphRDD.coalesce(this.workers);\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 23,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 808,
                        "startLineNumber": 158,
                        "startColumnNumber": 20,
                        "endLineNumber": 166,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n  if (this.workersSet && graphRDD.partitions().size() > this.workers)   graphRDD=graphRDD.coalesce(this.workers);\n  graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 61,
                    "astHeight": 9
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 968,
                        "startLineNumber": 158,
                        "startColumnNumber": 16,
                        "endLineNumber": 168,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "try {\n  graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n  if (this.workersSet && graphRDD.partitions().size() > this.workers)   graphRDD=graphRDD.coalesce(this.workers);\n  graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n}\n catch (final InstantiationException|IllegalAccessException e) {\n  throw new IllegalStateException(e.getMessage(),e);\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 80,
                    "astHeight": 10
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                "nodePosition": {
                    "charLength": 62,
                    "startLineNumber": 162,
                    "startColumnNumber": 24,
                    "endLineNumber": 162,
                    "endColumnNumber": 86
                },
                "nodeContext": "this.workersSet && graphRDD.partitions().size() > this.workers",
                "nodeType": "InfixExpression",
                "astNodeNumber": 13,
                "astHeight": 5
            },
            "tokenLength": 1,
            "type": "int"
        },
        {
            "nodeContext": "this.workers",
            "nodeType": "FieldAccess",
            "nodePosition": {
                "charLength": 12,
                "startLineNumber": 163,
                "startColumnNumber": 53,
                "endLineNumber": 163,
                "endColumnNumber": 65
            },
            "astNodeNumber": 3,
            "astHeight": 2,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.Assignment,rightHandSide]",
                    "nodePosition": {
                        "charLength": 31,
                        "startLineNumber": 163,
                        "startColumnNumber": 35,
                        "endLineNumber": 163,
                        "endColumnNumber": 66
                    },
                    "nodeContext": "graphRDD.coalesce(this.workers)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 6,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 42,
                        "startLineNumber": 163,
                        "startColumnNumber": 24,
                        "endLineNumber": 163,
                        "endColumnNumber": 66
                    },
                    "nodeContext": "graphRDD=graphRDD.coalesce(this.workers)",
                    "nodeType": "Assignment",
                    "astNodeNumber": 8,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 43,
                        "startLineNumber": 163,
                        "startColumnNumber": 24,
                        "endLineNumber": 163,
                        "endColumnNumber": 67
                    },
                    "nodeContext": "graphRDD=graphRDD.coalesce(this.workers);\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 9,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 207,
                        "startLineNumber": 162,
                        "startColumnNumber": 20,
                        "endLineNumber": 163,
                        "endColumnNumber": 67
                    },
                    "nodeContext": "if (this.workersSet && graphRDD.partitions().size() > this.workers) graphRDD=graphRDD.coalesce(this.workers);\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 23,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 808,
                        "startLineNumber": 158,
                        "startColumnNumber": 20,
                        "endLineNumber": 166,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n  if (this.workersSet && graphRDD.partitions().size() > this.workers)   graphRDD=graphRDD.coalesce(this.workers);\n  graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 61,
                    "astHeight": 9
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 968,
                        "startLineNumber": 158,
                        "startColumnNumber": 16,
                        "endLineNumber": 168,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "try {\n  graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n  if (this.workersSet && graphRDD.partitions().size() > this.workers)   graphRDD=graphRDD.coalesce(this.workers);\n  graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n}\n catch (final InstantiationException|IllegalAccessException e) {\n  throw new IllegalStateException(e.getMessage(),e);\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 80,
                    "astHeight": 10
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                "nodePosition": {
                    "charLength": 43,
                    "startLineNumber": 163,
                    "startColumnNumber": 24,
                    "endLineNumber": 163,
                    "endColumnNumber": 67
                },
                "nodeContext": "graphRDD=graphRDD.coalesce(this.workers);\n",
                "nodeType": "ExpressionStatement",
                "astNodeNumber": 9,
                "astHeight": 5
            },
            "tokenLength": 1,
            "type": "int"
        }
    ],
    "positionList": [
        {
            "charLength": 12,
            "startLineNumber": 162,
            "startColumnNumber": 74,
            "endLineNumber": 162,
            "endColumnNumber": 86
        },
        {
            "charLength": 12,
            "startLineNumber": 163,
            "startColumnNumber": 53,
            "endLineNumber": 163,
            "endColumnNumber": 65
        }
    ],
    "layoutRelationDataList": [
        {
            "firstKey": 0,
            "secondKey": 1,
            "layout": 2
        },
        {
            "firstKey": 1,
            "secondKey": 0,
            "layout": 3
        }
    ]
}