{
    "id": 197,
    "expression": "OutputRDD.class",
    "projectName": "tinkerpop",
    "commitID": "cf2e3b1a99dee9e45540859202cf784913658e47",
    "filePath": "spark-gremlin/src/main/java/org/apache/tinkerpop/gremlin/spark/process/computer/SparkGraphComputer.java",
    "occurrences": 2,
    "isArithmeticExpression": 1,
    "isGetTypeMethod": 0,
    "expressionList": [
        {
            "nodeContext": "OutputRDD.class",
            "nodeType": "TypeLiteral",
            "nodePosition": {
                "charLength": 15,
                "startLineNumber": 203,
                "startColumnNumber": 122,
                "endLineNumber": 203,
                "endColumnNumber": 137
            },
            "astNodeNumber": 3,
            "astHeight": 3,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                    "nodePosition": {
                        "charLength": 110,
                        "startLineNumber": 203,
                        "startColumnNumber": 28,
                        "endLineNumber": 203,
                        "endColumnNumber": 138
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 12,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                    "nodePosition": {
                        "charLength": 161,
                        "startLineNumber": 203,
                        "startColumnNumber": 28,
                        "endLineNumber": 204,
                        "endColumnNumber": 50
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance()",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 14,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 243,
                        "startLineNumber": 203,
                        "startColumnNumber": 28,
                        "endLineNumber": 205,
                        "endColumnNumber": 81
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 18,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 244,
                        "startLineNumber": 203,
                        "startColumnNumber": 28,
                        "endLineNumber": 205,
                        "endColumnNumber": 82
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 19,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 300,
                        "startLineNumber": 202,
                        "startColumnNumber": 28,
                        "endLineNumber": 206,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "{\n  hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 20,
                    "astHeight": 8
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 476,
                        "startLineNumber": 202,
                        "startColumnNumber": 24,
                        "endLineNumber": 208,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "try {\n  hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n}\n catch (final InstantiationException|IllegalAccessException e) {\n  throw new IllegalStateException(e.getMessage(),e);\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 39,
                    "astHeight": 9
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 524,
                        "startLineNumber": 201,
                        "startColumnNumber": 81,
                        "endLineNumber": 209,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "{\n  try {\n    hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 40,
                    "astHeight": 10
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 809,
                        "startLineNumber": 199,
                        "startColumnNumber": 20,
                        "endLineNumber": 209,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n  try {\n    hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 73,
                    "astHeight": 11
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 2548,
                        "startLineNumber": 175,
                        "startColumnNumber": 48,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 216,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 2580,
                        "startLineNumber": 175,
                        "startColumnNumber": 16,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (null != this.vertexProgram) {\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 222,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                "nodePosition": {
                    "charLength": 110,
                    "startLineNumber": 203,
                    "startColumnNumber": 28,
                    "endLineNumber": 203,
                    "endColumnNumber": 138
                },
                "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class)",
                "nodeType": "MethodInvocation",
                "astNodeNumber": 12,
                "astHeight": 4
            },
            "tokenLength": 1,
            "type": "java.lang.Class<org.apache.tinkerpop.gremlin.spark.structure.io.OutputRDD>"
        },
        {
            "nodeContext": "OutputRDD.class",
            "nodeType": "TypeLiteral",
            "nodePosition": {
                "charLength": 15,
                "startLineNumber": 234,
                "startColumnNumber": 130,
                "endLineNumber": 234,
                "endColumnNumber": 145
            },
            "astNodeNumber": 3,
            "astHeight": 3,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                    "nodePosition": {
                        "charLength": 110,
                        "startLineNumber": 234,
                        "startColumnNumber": 36,
                        "endLineNumber": 234,
                        "endColumnNumber": 146
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 12,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                    "nodePosition": {
                        "charLength": 169,
                        "startLineNumber": 234,
                        "startColumnNumber": 36,
                        "endLineNumber": 235,
                        "endColumnNumber": 58
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance()",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 14,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 316,
                        "startLineNumber": 234,
                        "startColumnNumber": 36,
                        "endLineNumber": 236,
                        "endColumnNumber": 146
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 26,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 394,
                        "startLineNumber": 233,
                        "startColumnNumber": 28,
                        "endLineNumber": 236,
                        "endColumnNumber": 147
                    },
                    "nodeContext": "mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD))",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 30,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 395,
                        "startLineNumber": 233,
                        "startColumnNumber": 28,
                        "endLineNumber": 236,
                        "endColumnNumber": 148
                    },
                    "nodeContext": "mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 31,
                    "astHeight": 8
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 451,
                        "startLineNumber": 232,
                        "startColumnNumber": 28,
                        "endLineNumber": 237,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "{\n  mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 32,
                    "astHeight": 9
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 627,
                        "startLineNumber": 232,
                        "startColumnNumber": 24,
                        "endLineNumber": 239,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "try {\n  mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n}\n catch (final InstantiationException|IllegalAccessException e) {\n  throw new IllegalStateException(e.getMessage(),e);\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 51,
                    "astHeight": 10
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
                    "nodePosition": {
                        "charLength": 1450,
                        "startLineNumber": 222,
                        "startColumnNumber": 71,
                        "endLineNumber": 240,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "{\n  final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n  mapReduce.storeState(newApacheConfiguration);\n  final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n  final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n  try {\n    mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 105,
                    "astHeight": 11
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 1501,
                        "startLineNumber": 222,
                        "startColumnNumber": 20,
                        "endLineNumber": 240,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "for (final MapReduce mapReduce : this.mapReducers) {\n  final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n  mapReduce.storeState(newApacheConfiguration);\n  final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n  final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n  try {\n    mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n}\n",
                    "nodeType": "EnhancedForStatement",
                    "astNodeNumber": 114,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 1931,
                        "startLineNumber": 217,
                        "startColumnNumber": 49,
                        "endLineNumber": 242,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n    vertexWritable.get().dropEdges();\n    return vertexWritable;\n  }\n).cache();\n  for (  final MapReduce mapReduce : this.mapReducers) {\n    final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n    mapReduce.storeState(newApacheConfiguration);\n    final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n    final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n    try {\n      mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n  mapReduceGraphRDD.unpersist();\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 147,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 1964,
                        "startLineNumber": 217,
                        "startColumnNumber": 16,
                        "endLineNumber": 242,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (!this.mapReducers.isEmpty()) {\n  final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n    vertexWritable.get().dropEdges();\n    return vertexWritable;\n  }\n).cache();\n  for (  final MapReduce mapReduce : this.mapReducers) {\n    final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n    mapReduce.storeState(newApacheConfiguration);\n    final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n    final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n    try {\n      mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n  mapReduceGraphRDD.unpersist();\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 154,
                    "astHeight": 14
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                "nodePosition": {
                    "charLength": 110,
                    "startLineNumber": 234,
                    "startColumnNumber": 36,
                    "endLineNumber": 234,
                    "endColumnNumber": 146
                },
                "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class)",
                "nodeType": "MethodInvocation",
                "astNodeNumber": 12,
                "astHeight": 4
            },
            "tokenLength": 1,
            "type": "java.lang.Class<org.apache.tinkerpop.gremlin.spark.structure.io.OutputRDD>"
        }
    ],
    "positionList": [
        {
            "charLength": 15,
            "startLineNumber": 203,
            "startColumnNumber": 122,
            "endLineNumber": 203,
            "endColumnNumber": 137
        },
        {
            "charLength": 15,
            "startLineNumber": 234,
            "startColumnNumber": 130,
            "endLineNumber": 234,
            "endColumnNumber": 145
        }
    ],
    "layoutRelationDataList": [
        {
            "firstKey": 0,
            "secondKey": 1,
            "layout": 10
        },
        {
            "firstKey": 1,
            "secondKey": 0,
            "layout": 11
        }
    ]
}