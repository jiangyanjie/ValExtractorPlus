{
    "id": 220,
    "expression": "Constants",
    "projectName": "tinkerpop",
    "commitID": "cf2e3b1a99dee9e45540859202cf784913658e47",
    "filePath": "spark-gremlin/src/main/java/org/apache/tinkerpop/gremlin/spark/process/computer/SparkGraphComputer.java",
    "occurrences": 14,
    "isArithmeticExpression": 1,
    "isGetTypeMethod": 0,
    "expressionList": [
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 136,
                "startColumnNumber": 66,
                "endLineNumber": 136,
                "endColumnNumber": 75
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 40,
                        "startLineNumber": 136,
                        "startColumnNumber": 66,
                        "endLineNumber": 136,
                        "endColumnNumber": 106
                    },
                    "nodeContext": "Constants.GREMLIN_HADOOP_OUTPUT_LOCATION",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.VariableDeclarationFragment,initializer]",
                    "nodePosition": {
                        "charLength": 71,
                        "startLineNumber": 136,
                        "startColumnNumber": 42,
                        "endLineNumber": 136,
                        "endColumnNumber": 113
                    },
                    "nodeContext": "hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 7,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.VariableDeclarationStatement,fragments]",
                    "nodePosition": {
                        "charLength": 88,
                        "startLineNumber": 136,
                        "startColumnNumber": 25,
                        "endLineNumber": 136,
                        "endColumnNumber": 113
                    },
                    "nodeContext": "outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null)",
                    "nodeType": "VariableDeclarationFragment",
                    "astNodeNumber": 9,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 102,
                        "startLineNumber": 136,
                        "startColumnNumber": 12,
                        "endLineNumber": 136,
                        "endColumnNumber": 114
                    },
                    "nodeContext": "final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n",
                    "nodeType": "VariableDeclarationStatement",
                    "astNodeNumber": 13,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                "nodePosition": {
                    "charLength": 102,
                    "startLineNumber": 136,
                    "startColumnNumber": 12,
                    "endLineNumber": 136,
                    "endColumnNumber": 114
                },
                "nodeContext": "final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n",
                "nodeType": "VariableDeclarationStatement",
                "astNodeNumber": 13,
                "astHeight": 5
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 145,
                "startColumnNumber": 42,
                "endLineNumber": 145,
                "endColumnNumber": 51
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.InfixExpression,leftOperand]",
                    "nodePosition": {
                        "charLength": 41,
                        "startLineNumber": 145,
                        "startColumnNumber": 42,
                        "endLineNumber": 145,
                        "endColumnNumber": 83
                    },
                    "nodeContext": "Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 145,
                        "startLineNumber": 145,
                        "startColumnNumber": 42,
                        "endLineNumber": 145,
                        "endColumnNumber": 187
                    },
                    "nodeContext": "Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\"",
                    "nodeType": "InfixExpression",
                    "astNodeNumber": 20,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 176,
                        "startLineNumber": 145,
                        "startColumnNumber": 12,
                        "endLineNumber": 145,
                        "endColumnNumber": 188
                    },
                    "nodeContext": "sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\")",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 23,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 177,
                        "startLineNumber": 145,
                        "startColumnNumber": 12,
                        "endLineNumber": 145,
                        "endColumnNumber": 189
                    },
                    "nodeContext": "sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 24,
                    "astHeight": 8
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                "nodePosition": {
                    "charLength": 177,
                    "startLineNumber": 145,
                    "startColumnNumber": 12,
                    "endLineNumber": 145,
                    "endColumnNumber": 189
                },
                "nodeContext": "sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n",
                "nodeType": "ExpressionStatement",
                "astNodeNumber": 24,
                "astHeight": 8
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 159,
                "startColumnNumber": 60,
                "endLineNumber": 159,
                "endColumnNumber": 69
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 39,
                        "startLineNumber": 159,
                        "startColumnNumber": 60,
                        "endLineNumber": 159,
                        "endColumnNumber": 99
                    },
                    "nodeContext": "Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                    "nodePosition": {
                        "charLength": 107,
                        "startLineNumber": 159,
                        "startColumnNumber": 31,
                        "endLineNumber": 159,
                        "endColumnNumber": 138
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 12,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                    "nodePosition": {
                        "charLength": 150,
                        "startLineNumber": 159,
                        "startColumnNumber": 31,
                        "endLineNumber": 160,
                        "endColumnNumber": 42
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance()",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 14,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.Assignment,rightHandSide]",
                    "nodePosition": {
                        "charLength": 227,
                        "startLineNumber": 159,
                        "startColumnNumber": 31,
                        "endLineNumber": 161,
                        "endColumnNumber": 76
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 18,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 238,
                        "startLineNumber": 159,
                        "startColumnNumber": 20,
                        "endLineNumber": 161,
                        "endColumnNumber": 76
                    },
                    "nodeContext": "graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext)",
                    "nodeType": "Assignment",
                    "astNodeNumber": 20,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 239,
                        "startLineNumber": 159,
                        "startColumnNumber": 20,
                        "endLineNumber": 161,
                        "endColumnNumber": 77
                    },
                    "nodeContext": "graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 21,
                    "astHeight": 8
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 808,
                        "startLineNumber": 158,
                        "startColumnNumber": 20,
                        "endLineNumber": 166,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n  if (this.workersSet && graphRDD.partitions().size() > this.workers)   graphRDD=graphRDD.coalesce(this.workers);\n  graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 61,
                    "astHeight": 9
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 968,
                        "startLineNumber": 158,
                        "startColumnNumber": 16,
                        "endLineNumber": 168,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "try {\n  graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n  if (this.workersSet && graphRDD.partitions().size() > this.workers)   graphRDD=graphRDD.coalesce(this.workers);\n  graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n}\n catch (final InstantiationException|IllegalAccessException e) {\n  throw new IllegalStateException(e.getMessage(),e);\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 80,
                    "astHeight": 10
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                "nodePosition": {
                    "charLength": 107,
                    "startLineNumber": 159,
                    "startColumnNumber": 31,
                    "endLineNumber": 159,
                    "endColumnNumber": 138
                },
                "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class)",
                "nodeType": "MethodInvocation",
                "astNodeNumber": 12,
                "astHeight": 4
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 165,
                "startColumnNumber": 96,
                "endLineNumber": 165,
                "endColumnNumber": 105
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 43,
                        "startLineNumber": 165,
                        "startColumnNumber": 96,
                        "endLineNumber": 165,
                        "endColumnNumber": 139
                    },
                    "nodeContext": "Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 83,
                        "startLineNumber": 165,
                        "startColumnNumber": 72,
                        "endLineNumber": 165,
                        "endColumnNumber": 155
                    },
                    "nodeContext": "hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 7,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 108,
                        "startLineNumber": 165,
                        "startColumnNumber": 48,
                        "endLineNumber": 165,
                        "endColumnNumber": 156
                    },
                    "nodeContext": "StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\"))",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 10,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.Assignment,rightHandSide]",
                    "nodePosition": {
                        "charLength": 126,
                        "startLineNumber": 165,
                        "startColumnNumber": 31,
                        "endLineNumber": 165,
                        "endColumnNumber": 157
                    },
                    "nodeContext": "graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")))",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 13,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 137,
                        "startLineNumber": 165,
                        "startColumnNumber": 20,
                        "endLineNumber": 165,
                        "endColumnNumber": 157
                    },
                    "nodeContext": "graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")))",
                    "nodeType": "Assignment",
                    "astNodeNumber": 15,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 138,
                        "startLineNumber": 165,
                        "startColumnNumber": 20,
                        "endLineNumber": 165,
                        "endColumnNumber": 158
                    },
                    "nodeContext": "graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 16,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 808,
                        "startLineNumber": 158,
                        "startColumnNumber": 20,
                        "endLineNumber": 166,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n  if (this.workersSet && graphRDD.partitions().size() > this.workers)   graphRDD=graphRDD.coalesce(this.workers);\n  graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 61,
                    "astHeight": 9
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 968,
                        "startLineNumber": 158,
                        "startColumnNumber": 16,
                        "endLineNumber": 168,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "try {\n  graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n  if (this.workersSet && graphRDD.partitions().size() > this.workers)   graphRDD=graphRDD.coalesce(this.workers);\n  graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n}\n catch (final InstantiationException|IllegalAccessException e) {\n  throw new IllegalStateException(e.getMessage(),e);\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 80,
                    "astHeight": 10
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                "nodePosition": {
                    "charLength": 138,
                    "startLineNumber": 165,
                    "startColumnNumber": 20,
                    "endLineNumber": 165,
                    "endColumnNumber": 158
                },
                "nodeContext": "graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n",
                "nodeType": "ExpressionStatement",
                "astNodeNumber": 16,
                "astHeight": 7
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 199,
                "startColumnNumber": 49,
                "endLineNumber": 199,
                "endColumnNumber": 58
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 44,
                        "startLineNumber": 199,
                        "startColumnNumber": 49,
                        "endLineNumber": 199,
                        "endColumnNumber": 93
                    },
                    "nodeContext": "Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.InfixExpression,leftOperand]",
                    "nodePosition": {
                        "charLength": 75,
                        "startLineNumber": 199,
                        "startColumnNumber": 25,
                        "endLineNumber": 199,
                        "endColumnNumber": 100
                    },
                    "nodeContext": "hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 7,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.InfixExpression,leftOperand]",
                    "nodePosition": {
                        "charLength": 83,
                        "startLineNumber": 199,
                        "startColumnNumber": 25,
                        "endLineNumber": 199,
                        "endColumnNumber": 108
                    },
                    "nodeContext": "hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null",
                    "nodeType": "InfixExpression",
                    "astNodeNumber": 9,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ParenthesizedExpression,expression]",
                    "nodePosition": {
                        "charLength": 194,
                        "startLineNumber": 199,
                        "startColumnNumber": 25,
                        "endLineNumber": 200,
                        "endColumnNumber": 107
                    },
                    "nodeContext": "hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null",
                    "nodeType": "InfixExpression",
                    "astNodeNumber": 19,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.InfixExpression,leftOperand]",
                    "nodePosition": {
                        "charLength": 196,
                        "startLineNumber": 199,
                        "startColumnNumber": 24,
                        "endLineNumber": 200,
                        "endColumnNumber": 108
                    },
                    "nodeContext": "(hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null)",
                    "nodeType": "ParenthesizedExpression",
                    "astNodeNumber": 20,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                    "nodePosition": {
                        "charLength": 279,
                        "startLineNumber": 199,
                        "startColumnNumber": 24,
                        "endLineNumber": 201,
                        "endColumnNumber": 79
                    },
                    "nodeContext": "(hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)",
                    "nodeType": "InfixExpression",
                    "astNodeNumber": 32,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 809,
                        "startLineNumber": 199,
                        "startColumnNumber": 20,
                        "endLineNumber": 209,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n  try {\n    hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 73,
                    "astHeight": 11
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 2548,
                        "startLineNumber": 175,
                        "startColumnNumber": 48,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 216,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 2580,
                        "startLineNumber": 175,
                        "startColumnNumber": 16,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (null != this.vertexProgram) {\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 222,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.InfixExpression,leftOperand]",
                "nodePosition": {
                    "charLength": 83,
                    "startLineNumber": 199,
                    "startColumnNumber": 25,
                    "endLineNumber": 199,
                    "endColumnNumber": 108
                },
                "nodeContext": "hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null",
                "nodeType": "InfixExpression",
                "astNodeNumber": 9,
                "astHeight": 4
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 200,
                "startColumnNumber": 52,
                "endLineNumber": 200,
                "endColumnNumber": 61
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 40,
                        "startLineNumber": 200,
                        "startColumnNumber": 52,
                        "endLineNumber": 200,
                        "endColumnNumber": 92
                    },
                    "nodeContext": "Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.InfixExpression,leftOperand]",
                    "nodePosition": {
                        "charLength": 71,
                        "startLineNumber": 200,
                        "startColumnNumber": 28,
                        "endLineNumber": 200,
                        "endColumnNumber": 99
                    },
                    "nodeContext": "hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 7,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.InfixExpression,rightOperand]",
                    "nodePosition": {
                        "charLength": 79,
                        "startLineNumber": 200,
                        "startColumnNumber": 28,
                        "endLineNumber": 200,
                        "endColumnNumber": 107
                    },
                    "nodeContext": "hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null",
                    "nodeType": "InfixExpression",
                    "astNodeNumber": 9,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ParenthesizedExpression,expression]",
                    "nodePosition": {
                        "charLength": 194,
                        "startLineNumber": 199,
                        "startColumnNumber": 25,
                        "endLineNumber": 200,
                        "endColumnNumber": 107
                    },
                    "nodeContext": "hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null",
                    "nodeType": "InfixExpression",
                    "astNodeNumber": 19,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.InfixExpression,leftOperand]",
                    "nodePosition": {
                        "charLength": 196,
                        "startLineNumber": 199,
                        "startColumnNumber": 24,
                        "endLineNumber": 200,
                        "endColumnNumber": 108
                    },
                    "nodeContext": "(hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null)",
                    "nodeType": "ParenthesizedExpression",
                    "astNodeNumber": 20,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                    "nodePosition": {
                        "charLength": 279,
                        "startLineNumber": 199,
                        "startColumnNumber": 24,
                        "endLineNumber": 201,
                        "endColumnNumber": 79
                    },
                    "nodeContext": "(hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)",
                    "nodeType": "InfixExpression",
                    "astNodeNumber": 32,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 809,
                        "startLineNumber": 199,
                        "startColumnNumber": 20,
                        "endLineNumber": 209,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n  try {\n    hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 73,
                    "astHeight": 11
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 2548,
                        "startLineNumber": 175,
                        "startColumnNumber": 48,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 216,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 2580,
                        "startLineNumber": 175,
                        "startColumnNumber": 16,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (null != this.vertexProgram) {\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 222,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.InfixExpression,rightOperand]",
                "nodePosition": {
                    "charLength": 79,
                    "startLineNumber": 200,
                    "startColumnNumber": 28,
                    "endLineNumber": 200,
                    "endColumnNumber": 107
                },
                "nodeContext": "hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null",
                "nodeType": "InfixExpression",
                "astNodeNumber": 9,
                "astHeight": 4
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 203,
                "startColumnNumber": 57,
                "endLineNumber": 203,
                "endColumnNumber": 66
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 40,
                        "startLineNumber": 203,
                        "startColumnNumber": 57,
                        "endLineNumber": 203,
                        "endColumnNumber": 97
                    },
                    "nodeContext": "Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                    "nodePosition": {
                        "charLength": 110,
                        "startLineNumber": 203,
                        "startColumnNumber": 28,
                        "endLineNumber": 203,
                        "endColumnNumber": 138
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 12,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                    "nodePosition": {
                        "charLength": 161,
                        "startLineNumber": 203,
                        "startColumnNumber": 28,
                        "endLineNumber": 204,
                        "endColumnNumber": 50
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance()",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 14,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 243,
                        "startLineNumber": 203,
                        "startColumnNumber": 28,
                        "endLineNumber": 205,
                        "endColumnNumber": 81
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 18,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 244,
                        "startLineNumber": 203,
                        "startColumnNumber": 28,
                        "endLineNumber": 205,
                        "endColumnNumber": 82
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 19,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 300,
                        "startLineNumber": 202,
                        "startColumnNumber": 28,
                        "endLineNumber": 206,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "{\n  hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 20,
                    "astHeight": 8
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 476,
                        "startLineNumber": 202,
                        "startColumnNumber": 24,
                        "endLineNumber": 208,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "try {\n  hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n}\n catch (final InstantiationException|IllegalAccessException e) {\n  throw new IllegalStateException(e.getMessage(),e);\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 39,
                    "astHeight": 9
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 524,
                        "startLineNumber": 201,
                        "startColumnNumber": 81,
                        "endLineNumber": 209,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "{\n  try {\n    hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 40,
                    "astHeight": 10
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 809,
                        "startLineNumber": 199,
                        "startColumnNumber": 20,
                        "endLineNumber": 209,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n  try {\n    hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 73,
                    "astHeight": 11
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 2548,
                        "startLineNumber": 175,
                        "startColumnNumber": 48,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 216,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 2580,
                        "startLineNumber": 175,
                        "startColumnNumber": 16,
                        "endLineNumber": 210,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (null != this.vertexProgram) {\n  memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n  this.vertexProgram.setup(memory);\n  memory.broadcastMemory(sparkContext);\n  final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n  this.vertexProgram.storeState(vertexProgramConfiguration);\n  ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n  ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n  while (true) {\n    memory.setInTask(true);\n    viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n    memory.setInTask(false);\n    if (this.vertexProgram.terminate(memory))     break;\n else {\n      memory.incrIteration();\n      memory.broadcastMemory(sparkContext);\n    }\n  }\n  final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n  graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n  if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    try {\n      hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 222,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                "nodePosition": {
                    "charLength": 110,
                    "startLineNumber": 203,
                    "startColumnNumber": 28,
                    "endLineNumber": 203,
                    "endColumnNumber": 138
                },
                "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class)",
                "nodeType": "MethodInvocation",
                "astNodeNumber": 12,
                "astHeight": 4
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 234,
                "startColumnNumber": 65,
                "endLineNumber": 234,
                "endColumnNumber": 74
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 40,
                        "startLineNumber": 234,
                        "startColumnNumber": 65,
                        "endLineNumber": 234,
                        "endColumnNumber": 105
                    },
                    "nodeContext": "Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                    "nodePosition": {
                        "charLength": 110,
                        "startLineNumber": 234,
                        "startColumnNumber": 36,
                        "endLineNumber": 234,
                        "endColumnNumber": 146
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 12,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                    "nodePosition": {
                        "charLength": 169,
                        "startLineNumber": 234,
                        "startColumnNumber": 36,
                        "endLineNumber": 235,
                        "endColumnNumber": 58
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance()",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 14,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 316,
                        "startLineNumber": 234,
                        "startColumnNumber": 36,
                        "endLineNumber": 236,
                        "endColumnNumber": 146
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 26,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 394,
                        "startLineNumber": 233,
                        "startColumnNumber": 28,
                        "endLineNumber": 236,
                        "endColumnNumber": 147
                    },
                    "nodeContext": "mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD))",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 30,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 395,
                        "startLineNumber": 233,
                        "startColumnNumber": 28,
                        "endLineNumber": 236,
                        "endColumnNumber": 148
                    },
                    "nodeContext": "mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 31,
                    "astHeight": 8
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 451,
                        "startLineNumber": 232,
                        "startColumnNumber": 28,
                        "endLineNumber": 237,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "{\n  mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 32,
                    "astHeight": 9
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 627,
                        "startLineNumber": 232,
                        "startColumnNumber": 24,
                        "endLineNumber": 239,
                        "endColumnNumber": 25
                    },
                    "nodeContext": "try {\n  mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n}\n catch (final InstantiationException|IllegalAccessException e) {\n  throw new IllegalStateException(e.getMessage(),e);\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 51,
                    "astHeight": 10
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
                    "nodePosition": {
                        "charLength": 1450,
                        "startLineNumber": 222,
                        "startColumnNumber": 71,
                        "endLineNumber": 240,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "{\n  final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n  mapReduce.storeState(newApacheConfiguration);\n  final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n  final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n  try {\n    mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 105,
                    "astHeight": 11
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 1501,
                        "startLineNumber": 222,
                        "startColumnNumber": 20,
                        "endLineNumber": 240,
                        "endColumnNumber": 21
                    },
                    "nodeContext": "for (final MapReduce mapReduce : this.mapReducers) {\n  final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n  mapReduce.storeState(newApacheConfiguration);\n  final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n  final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n  try {\n    mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n}\n",
                    "nodeType": "EnhancedForStatement",
                    "astNodeNumber": 114,
                    "astHeight": 12
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 1931,
                        "startLineNumber": 217,
                        "startColumnNumber": 49,
                        "endLineNumber": 242,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n    vertexWritable.get().dropEdges();\n    return vertexWritable;\n  }\n).cache();\n  for (  final MapReduce mapReduce : this.mapReducers) {\n    final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n    mapReduce.storeState(newApacheConfiguration);\n    final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n    final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n    try {\n      mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n  mapReduceGraphRDD.unpersist();\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 147,
                    "astHeight": 13
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 1964,
                        "startLineNumber": 217,
                        "startColumnNumber": 16,
                        "endLineNumber": 242,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (!this.mapReducers.isEmpty()) {\n  final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n    vertexWritable.get().dropEdges();\n    return vertexWritable;\n  }\n).cache();\n  for (  final MapReduce mapReduce : this.mapReducers) {\n    final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n    mapReduce.storeState(newApacheConfiguration);\n    final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n    final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n    try {\n      mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n  }\n  mapReduceGraphRDD.unpersist();\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 154,
                    "astHeight": 14
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
                "nodePosition": {
                    "charLength": 110,
                    "startLineNumber": 234,
                    "startColumnNumber": 36,
                    "endLineNumber": 234,
                    "endColumnNumber": 146
                },
                "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class)",
                "nodeType": "MethodInvocation",
                "astNodeNumber": 12,
                "astHeight": 4
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 245,
                "startColumnNumber": 82,
                "endLineNumber": 245,
                "endColumnNumber": 91
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 40,
                        "startLineNumber": 245,
                        "startColumnNumber": 82,
                        "endLineNumber": 245,
                        "endColumnNumber": 122
                    },
                    "nodeContext": "Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 76,
                        "startLineNumber": 245,
                        "startColumnNumber": 53,
                        "endLineNumber": 245,
                        "endColumnNumber": 129
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 7,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.PrefixExpression,operand]",
                    "nodePosition": {
                        "charLength": 109,
                        "startLineNumber": 245,
                        "startColumnNumber": 21,
                        "endLineNumber": 245,
                        "endColumnNumber": 130
                    },
                    "nodeContext": "PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null))",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 12,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.InfixExpression,leftOperand]",
                    "nodePosition": {
                        "charLength": 110,
                        "startLineNumber": 245,
                        "startColumnNumber": 20,
                        "endLineNumber": 245,
                        "endColumnNumber": 130
                    },
                    "nodeContext": "!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null))",
                    "nodeType": "PrefixExpression",
                    "astNodeNumber": 13,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                    "nodePosition": {
                        "charLength": 164,
                        "startLineNumber": 245,
                        "startColumnNumber": 20,
                        "endLineNumber": 245,
                        "endColumnNumber": 184
                    },
                    "nodeContext": "!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)",
                    "nodeType": "InfixExpression",
                    "astNodeNumber": 24,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 458,
                        "startLineNumber": 245,
                        "startColumnNumber": 16,
                        "endLineNumber": 249,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n  graphRDD.unpersist();\n  if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))   SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 49,
                    "astHeight": 8
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                "nodePosition": {
                    "charLength": 164,
                    "startLineNumber": 245,
                    "startColumnNumber": 20,
                    "endLineNumber": 245,
                    "endColumnNumber": 184
                },
                "nodeContext": "!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)",
                "nodeType": "InfixExpression",
                "astNodeNumber": 24,
                "astHeight": 6
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 247,
                "startColumnNumber": 56,
                "endLineNumber": 247,
                "endColumnNumber": 65
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 40,
                        "startLineNumber": 247,
                        "startColumnNumber": 56,
                        "endLineNumber": 247,
                        "endColumnNumber": 96
                    },
                    "nodeContext": "Constants.GREMLIN_HADOOP_OUTPUT_LOCATION",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                    "nodePosition": {
                        "charLength": 73,
                        "startLineNumber": 247,
                        "startColumnNumber": 24,
                        "endLineNumber": 247,
                        "endColumnNumber": 97
                    },
                    "nodeContext": "apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 6,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 206,
                        "startLineNumber": 247,
                        "startColumnNumber": 20,
                        "endLineNumber": 248,
                        "endColumnNumber": 127
                    },
                    "nodeContext": "if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION)) SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 19,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 288,
                        "startLineNumber": 245,
                        "startColumnNumber": 186,
                        "endLineNumber": 249,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  graphRDD.unpersist();\n  if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))   SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 24,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 458,
                        "startLineNumber": 245,
                        "startColumnNumber": 16,
                        "endLineNumber": 249,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n  graphRDD.unpersist();\n  if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))   SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 49,
                    "astHeight": 8
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                "nodePosition": {
                    "charLength": 73,
                    "startLineNumber": 247,
                    "startColumnNumber": 24,
                    "endLineNumber": 247,
                    "endColumnNumber": 97
                },
                "nodeContext": "apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION)",
                "nodeType": "MethodInvocation",
                "astNodeNumber": 6,
                "astHeight": 3
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 248,
                "startColumnNumber": 84,
                "endLineNumber": 248,
                "endColumnNumber": 93
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 40,
                        "startLineNumber": 248,
                        "startColumnNumber": 84,
                        "endLineNumber": 248,
                        "endColumnNumber": 124
                    },
                    "nodeContext": "Constants.GREMLIN_HADOOP_OUTPUT_LOCATION",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 71,
                        "startLineNumber": 248,
                        "startColumnNumber": 54,
                        "endLineNumber": 248,
                        "endColumnNumber": 125
                    },
                    "nodeContext": "apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 6,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 102,
                        "startLineNumber": 248,
                        "startColumnNumber": 24,
                        "endLineNumber": 248,
                        "endColumnNumber": 126
                    },
                    "nodeContext": "SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 11,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 103,
                        "startLineNumber": 248,
                        "startColumnNumber": 24,
                        "endLineNumber": 248,
                        "endColumnNumber": 127
                    },
                    "nodeContext": "SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 12,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 206,
                        "startLineNumber": 247,
                        "startColumnNumber": 20,
                        "endLineNumber": 248,
                        "endColumnNumber": 127
                    },
                    "nodeContext": "if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION)) SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 19,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 288,
                        "startLineNumber": 245,
                        "startColumnNumber": 186,
                        "endLineNumber": 249,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "{\n  graphRDD.unpersist();\n  if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))   SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 24,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 458,
                        "startLineNumber": 245,
                        "startColumnNumber": 16,
                        "endLineNumber": 249,
                        "endColumnNumber": 17
                    },
                    "nodeContext": "if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n  graphRDD.unpersist();\n  if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))   SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n}\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 49,
                    "astHeight": 8
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                "nodePosition": {
                    "charLength": 103,
                    "startLineNumber": 248,
                    "startColumnNumber": 24,
                    "endLineNumber": 248,
                    "endColumnNumber": 127
                },
                "nodeContext": "SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n",
                "nodeType": "ExpressionStatement",
                "astNodeNumber": 12,
                "astHeight": 5
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 251,
                "startColumnNumber": 89,
                "endLineNumber": 251,
                "endColumnNumber": 98
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 44,
                        "startLineNumber": 251,
                        "startColumnNumber": 89,
                        "endLineNumber": 251,
                        "endColumnNumber": 133
                    },
                    "nodeContext": "Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 97,
                        "startLineNumber": 251,
                        "startColumnNumber": 60,
                        "endLineNumber": 251,
                        "endColumnNumber": 157
                    },
                    "nodeContext": "hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 9,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.InfixExpression,leftOperand]",
                    "nodePosition": {
                        "charLength": 138,
                        "startLineNumber": 251,
                        "startColumnNumber": 20,
                        "endLineNumber": 251,
                        "endColumnNumber": 158
                    },
                    "nodeContext": "FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class))",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 14,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                    "nodePosition": {
                        "charLength": 192,
                        "startLineNumber": 251,
                        "startColumnNumber": 20,
                        "endLineNumber": 251,
                        "endColumnNumber": 212
                    },
                    "nodeContext": "FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING)",
                    "nodeType": "InfixExpression",
                    "astNodeNumber": 25,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 338,
                        "startLineNumber": 251,
                        "startColumnNumber": 16,
                        "endLineNumber": 252,
                        "endColumnNumber": 140
                    },
                    "nodeContext": "if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING)) FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 39,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                "nodePosition": {
                    "charLength": 192,
                    "startLineNumber": 251,
                    "startColumnNumber": 20,
                    "endLineNumber": 251,
                    "endColumnNumber": 212
                },
                "nodeContext": "FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING)",
                "nodeType": "InfixExpression",
                "astNodeNumber": 25,
                "astHeight": 6
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 252,
                "startColumnNumber": 97,
                "endLineNumber": 252,
                "endColumnNumber": 106
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 40,
                        "startLineNumber": 252,
                        "startColumnNumber": 97,
                        "endLineNumber": 252,
                        "endColumnNumber": 137
                    },
                    "nodeContext": "Constants.GREMLIN_HADOOP_OUTPUT_LOCATION",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 71,
                        "startLineNumber": 252,
                        "startColumnNumber": 67,
                        "endLineNumber": 252,
                        "endColumnNumber": 138
                    },
                    "nodeContext": "apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 6,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
                    "nodePosition": {
                        "charLength": 119,
                        "startLineNumber": 252,
                        "startColumnNumber": 20,
                        "endLineNumber": 252,
                        "endColumnNumber": 139
                    },
                    "nodeContext": "FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 12,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                    "nodePosition": {
                        "charLength": 120,
                        "startLineNumber": 252,
                        "startColumnNumber": 20,
                        "endLineNumber": 252,
                        "endColumnNumber": 140
                    },
                    "nodeContext": "FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n",
                    "nodeType": "ExpressionStatement",
                    "astNodeNumber": 13,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 338,
                        "startLineNumber": 251,
                        "startColumnNumber": 16,
                        "endLineNumber": 252,
                        "endColumnNumber": 140
                    },
                    "nodeContext": "if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING)) FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 39,
                    "astHeight": 7
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
                    "nodePosition": {
                        "charLength": 7907,
                        "startLineNumber": 150,
                        "startColumnNumber": 16,
                        "endLineNumber": 256,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 645,
                    "astHeight": 15
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
                "nodePosition": {
                    "charLength": 120,
                    "startLineNumber": 252,
                    "startColumnNumber": 20,
                    "endLineNumber": 252,
                    "endColumnNumber": 140
                },
                "nodeContext": "FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n",
                "nodeType": "ExpressionStatement",
                "astNodeNumber": 13,
                "astHeight": 5
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        },
        {
            "nodeContext": "Constants",
            "nodeType": "SimpleName",
            "nodePosition": {
                "charLength": 9,
                "startLineNumber": 257,
                "startColumnNumber": 52,
                "endLineNumber": 257,
                "endColumnNumber": 61
            },
            "astNodeNumber": 1,
            "astHeight": 1,
            "parentDataList": [
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 39,
                        "startLineNumber": 257,
                        "startColumnNumber": 52,
                        "endLineNumber": 257,
                        "endColumnNumber": 91
                    },
                    "nodeContext": "Constants.GREMLIN_SPARK_PERSIST_CONTEXT",
                    "nodeType": "QualifiedName",
                    "astNodeNumber": 3,
                    "astHeight": 2
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.PrefixExpression,operand]",
                    "nodePosition": {
                        "charLength": 78,
                        "startLineNumber": 257,
                        "startColumnNumber": 21,
                        "endLineNumber": 257,
                        "endColumnNumber": 99
                    },
                    "nodeContext": "apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false)",
                    "nodeType": "MethodInvocation",
                    "astNodeNumber": 7,
                    "astHeight": 3
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                    "nodePosition": {
                        "charLength": 79,
                        "startLineNumber": 257,
                        "startColumnNumber": 20,
                        "endLineNumber": 257,
                        "endColumnNumber": 99
                    },
                    "nodeContext": "!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false)",
                    "nodeType": "PrefixExpression",
                    "astNodeNumber": 8,
                    "astHeight": 4
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 119,
                        "startLineNumber": 257,
                        "startColumnNumber": 16,
                        "endLineNumber": 258,
                        "endColumnNumber": 34
                    },
                    "nodeContext": "if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false)) Spark.close();\n",
                    "nodeType": "IfStatement",
                    "astNodeNumber": 13,
                    "astHeight": 5
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,finally]",
                    "nodePosition": {
                        "charLength": 151,
                        "startLineNumber": 256,
                        "startColumnNumber": 22,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "{\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 14,
                    "astHeight": 6
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
                    "nodePosition": {
                        "charLength": 8071,
                        "startLineNumber": 150,
                        "startColumnNumber": 12,
                        "endLineNumber": 259,
                        "endColumnNumber": 13
                    },
                    "nodeContext": "try {\n  final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n  Spark.create(sparkContext.sc());\n  updateLocalConfiguration(sparkContext,sparkConfiguration);\n  this.loadJars(sparkContext,hadoopConfiguration);\n  JavaPairRDD<Object,VertexWritable> graphRDD;\n  try {\n    graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n    if (this.workersSet && graphRDD.partitions().size() > this.workers)     graphRDD=graphRDD.coalesce(this.workers);\n    graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n  }\n catch (  final InstantiationException|IllegalAccessException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n  if (null != this.vertexProgram) {\n    memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n    this.vertexProgram.setup(memory);\n    memory.broadcastMemory(sparkContext);\n    final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n    this.vertexProgram.storeState(vertexProgramConfiguration);\n    ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n    ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n    while (true) {\n      memory.setInTask(true);\n      viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n      memory.setInTask(false);\n      if (this.vertexProgram.terminate(memory))       break;\n else {\n        memory.incrIteration();\n        memory.broadcastMemory(sparkContext);\n      }\n    }\n    final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n    graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n    if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      try {\n        hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n  }\n  final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n  if (!this.mapReducers.isEmpty()) {\n    final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n      vertexWritable.get().dropEdges();\n      return vertexWritable;\n    }\n).cache();\n    for (    final MapReduce mapReduce : this.mapReducers) {\n      final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n      mapReduce.storeState(newApacheConfiguration);\n      final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n      final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n      try {\n        mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n      }\n catch (      final InstantiationException|IllegalAccessException e) {\n        throw new IllegalStateException(e.getMessage(),e);\n      }\n    }\n    mapReduceGraphRDD.unpersist();\n  }\n  if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n    graphRDD.unpersist();\n    if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))     SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  }\n  if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))   FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n  finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n  return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n}\n  finally {\n  if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))   Spark.close();\n}\n",
                    "nodeType": "TryStatement",
                    "astNodeNumber": 660,
                    "astHeight": 16
                },
                {
                    "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
                    "nodePosition": {
                        "charLength": 9309,
                        "startLineNumber": 132,
                        "startColumnNumber": 67,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "{\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "Block",
                    "astNodeNumber": 781,
                    "astHeight": 17
                },
                {
                    "locationInParent": "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
                    "nodePosition": {
                        "charLength": 9315,
                        "startLineNumber": 132,
                        "startColumnNumber": 61,
                        "endLineNumber": 260,
                        "endColumnNumber": 9
                    },
                    "nodeContext": "() -> {\n  final long startTime=System.currentTimeMillis();\n  SparkMemory memory=null;\n  final String outputLocation=hadoopConfiguration.get(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION,null);\n  try {\n    if (null != outputLocation && FileSystem.get(hadoopConfiguration).exists(new Path(outputLocation)))     FileSystem.get(hadoopConfiguration).delete(new Path(outputLocation),true);\n  }\n catch (  final IOException e) {\n    throw new IllegalStateException(e.getMessage(),e);\n  }\n  final SparkConf sparkConfiguration=new SparkConf();\n  sparkConfiguration.setAppName(Constants.GREMLIN_HADOOP_SPARK_JOB_PREFIX + (null == this.vertexProgram ? \"No VertexProgram\" : this.vertexProgram) + \"[\"+ this.mapReducers+ \"]\");\n  hadoopConfiguration.forEach(entry -> sparkConfiguration.set(entry.getKey(),entry.getValue()));\n  try {\n    final JavaSparkContext sparkContext=new JavaSparkContext(SparkContext.getOrCreate(sparkConfiguration));\n    Spark.create(sparkContext.sc());\n    updateLocalConfiguration(sparkContext,sparkConfiguration);\n    this.loadJars(sparkContext,hadoopConfiguration);\n    JavaPairRDD<Object,VertexWritable> graphRDD;\n    try {\n      graphRDD=hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_INPUT_RDD,InputFormatRDD.class,InputRDD.class).newInstance().readGraphRDD(apacheConfiguration,sparkContext);\n      if (this.workersSet && graphRDD.partitions().size() > this.workers)       graphRDD=graphRDD.coalesce(this.workers);\n      graphRDD=graphRDD.persist(StorageLevel.fromString(hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_STORAGE_LEVEL,\"MEMORY_ONLY\")));\n    }\n catch (    final InstantiationException|IllegalAccessException e) {\n      throw new IllegalStateException(e.getMessage(),e);\n    }\n    JavaPairRDD<Object,ViewIncomingPayload<Object>> viewIncomingRDD=null;\n    if (null != this.vertexProgram) {\n      memory=new SparkMemory(this.vertexProgram,this.mapReducers,sparkContext);\n      this.vertexProgram.setup(memory);\n      memory.broadcastMemory(sparkContext);\n      final HadoopConfiguration vertexProgramConfiguration=new HadoopConfiguration();\n      this.vertexProgram.storeState(vertexProgramConfiguration);\n      ConfigurationUtils.copy(vertexProgramConfiguration,apacheConfiguration);\n      ConfUtil.mergeApacheIntoHadoopConfiguration(vertexProgramConfiguration,hadoopConfiguration);\n      while (true) {\n        memory.setInTask(true);\n        viewIncomingRDD=SparkExecutor.executeVertexProgramIteration(graphRDD,viewIncomingRDD,memory,vertexProgramConfiguration);\n        memory.setInTask(false);\n        if (this.vertexProgram.terminate(memory))         break;\n else {\n          memory.incrIteration();\n          memory.broadcastMemory(sparkContext);\n        }\n      }\n      final String[] elementComputeKeys=this.vertexProgram == null ? new String[0] : this.vertexProgram.getElementComputeKeys().toArray(new String[this.vertexProgram.getElementComputeKeys().size()]);\n      graphRDD=SparkExecutor.prepareFinalGraphRDD(graphRDD,viewIncomingRDD,elementComputeKeys);\n      if ((hadoopConfiguration.get(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,null) != null || hadoopConfiguration.get(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null) != null) && !this.persist.equals(GraphComputer.Persist.NOTHING)) {\n        try {\n          hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeGraphRDD(apacheConfiguration,graphRDD);\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n    }\n    final Memory.Admin finalMemory=null == memory ? new MapMemory() : new MapMemory(memory);\n    if (!this.mapReducers.isEmpty()) {\n      final JavaPairRDD<Object,VertexWritable> mapReduceGraphRDD=graphRDD.mapValues(vertexWritable -> {\n        vertexWritable.get().dropEdges();\n        return vertexWritable;\n      }\n).cache();\n      for (      final MapReduce mapReduce : this.mapReducers) {\n        final HadoopConfiguration newApacheConfiguration=new HadoopConfiguration(apacheConfiguration);\n        mapReduce.storeState(newApacheConfiguration);\n        final JavaPairRDD mapRDD=SparkExecutor.executeMap((JavaPairRDD)mapReduceGraphRDD,mapReduce,newApacheConfiguration);\n        final JavaPairRDD reduceRDD=(mapReduce.doStage(MapReduce.Stage.REDUCE)) ? SparkExecutor.executeReduce(mapRDD,mapReduce,newApacheConfiguration) : null;\n        try {\n          mapReduce.addResultToMemory(finalMemory,hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,OutputFormatRDD.class,OutputRDD.class).newInstance().writeMemoryRDD(apacheConfiguration,mapReduce.getMemoryKey(),null == reduceRDD ? mapRDD : reduceRDD));\n        }\n catch (        final InstantiationException|IllegalAccessException e) {\n          throw new IllegalStateException(e.getMessage(),e);\n        }\n      }\n      mapReduceGraphRDD.unpersist();\n    }\n    if (!PersistedOutputRDD.class.equals(hadoopConfiguration.getClass(Constants.GREMLIN_SPARK_GRAPH_OUTPUT_RDD,null)) || this.persist.equals(GraphComputer.Persist.NOTHING)) {\n      graphRDD.unpersist();\n      if (apacheConfiguration.containsKey(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION))       SparkContextStorage.open().rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    }\n    if (FileOutputFormat.class.isAssignableFrom(hadoopConfiguration.getClass(Constants.GREMLIN_HADOOP_GRAPH_OUTPUT_FORMAT,FileInputFormat.class)) && this.persist.equals(GraphComputer.Persist.NOTHING))     FileSystemStorage.open(hadoopConfiguration).rm(apacheConfiguration.getString(Constants.GREMLIN_HADOOP_OUTPUT_LOCATION));\n    finalMemory.setRuntime(System.currentTimeMillis() - startTime);\n    return new DefaultComputerResult(InputOutputHelper.getOutputGraph(apacheConfiguration,this.resultGraph,this.persist),finalMemory.asImmutable());\n  }\n  finally {\n    if (!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false))     Spark.close();\n  }\n}\n",
                    "nodeType": "LambdaExpression",
                    "astNodeNumber": 782,
                    "astHeight": 18
                }
            ],
            "currentLineData": {
                "locationInParent": "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
                "nodePosition": {
                    "charLength": 79,
                    "startLineNumber": 257,
                    "startColumnNumber": 20,
                    "endLineNumber": 257,
                    "endColumnNumber": 99
                },
                "nodeContext": "!apacheConfiguration.getBoolean(Constants.GREMLIN_SPARK_PERSIST_CONTEXT,false)",
                "nodeType": "PrefixExpression",
                "astNodeNumber": 8,
                "astHeight": 4
            },
            "tokenLength": 1,
            "type": "org.apache.tinkerpop.gremlin.hadoop.Constants"
        }
    ],
    "positionList": [
        {
            "charLength": 9,
            "startLineNumber": 136,
            "startColumnNumber": 66,
            "endLineNumber": 136,
            "endColumnNumber": 75
        },
        {
            "charLength": 9,
            "startLineNumber": 145,
            "startColumnNumber": 42,
            "endLineNumber": 145,
            "endColumnNumber": 51
        },
        {
            "charLength": 9,
            "startLineNumber": 159,
            "startColumnNumber": 60,
            "endLineNumber": 159,
            "endColumnNumber": 69
        },
        {
            "charLength": 9,
            "startLineNumber": 165,
            "startColumnNumber": 96,
            "endLineNumber": 165,
            "endColumnNumber": 105
        },
        {
            "charLength": 9,
            "startLineNumber": 199,
            "startColumnNumber": 49,
            "endLineNumber": 199,
            "endColumnNumber": 58
        },
        {
            "charLength": 9,
            "startLineNumber": 200,
            "startColumnNumber": 52,
            "endLineNumber": 200,
            "endColumnNumber": 61
        },
        {
            "charLength": 9,
            "startLineNumber": 203,
            "startColumnNumber": 57,
            "endLineNumber": 203,
            "endColumnNumber": 66
        },
        {
            "charLength": 9,
            "startLineNumber": 234,
            "startColumnNumber": 65,
            "endLineNumber": 234,
            "endColumnNumber": 74
        },
        {
            "charLength": 9,
            "startLineNumber": 245,
            "startColumnNumber": 82,
            "endLineNumber": 245,
            "endColumnNumber": 91
        },
        {
            "charLength": 9,
            "startLineNumber": 247,
            "startColumnNumber": 56,
            "endLineNumber": 247,
            "endColumnNumber": 65
        },
        {
            "charLength": 9,
            "startLineNumber": 248,
            "startColumnNumber": 84,
            "endLineNumber": 248,
            "endColumnNumber": 93
        },
        {
            "charLength": 9,
            "startLineNumber": 251,
            "startColumnNumber": 89,
            "endLineNumber": 251,
            "endColumnNumber": 98
        },
        {
            "charLength": 9,
            "startLineNumber": 252,
            "startColumnNumber": 97,
            "endLineNumber": 252,
            "endColumnNumber": 106
        },
        {
            "charLength": 9,
            "startLineNumber": 257,
            "startColumnNumber": 52,
            "endLineNumber": 257,
            "endColumnNumber": 61
        }
    ],
    "layoutRelationDataList": [
        {
            "firstKey": 0,
            "secondKey": 1,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 2,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 3,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 4,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 5,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 6,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 7,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 8,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 9,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 10,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 11,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 12,
            "layout": 4
        },
        {
            "firstKey": 0,
            "secondKey": 13,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 0,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 2,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 3,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 4,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 5,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 6,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 7,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 8,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 9,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 10,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 11,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 12,
            "layout": 4
        },
        {
            "firstKey": 1,
            "secondKey": 13,
            "layout": 4
        },
        {
            "firstKey": 2,
            "secondKey": 0,
            "layout": 10
        },
        {
            "firstKey": 2,
            "secondKey": 1,
            "layout": 10
        },
        {
            "firstKey": 2,
            "secondKey": 3,
            "layout": 6
        },
        {
            "firstKey": 2,
            "secondKey": 4,
            "layout": 8
        },
        {
            "firstKey": 2,
            "secondKey": 5,
            "layout": 8
        },
        {
            "firstKey": 2,
            "secondKey": 6,
            "layout": 8
        },
        {
            "firstKey": 2,
            "secondKey": 7,
            "layout": 8
        },
        {
            "firstKey": 2,
            "secondKey": 8,
            "layout": 8
        },
        {
            "firstKey": 2,
            "secondKey": 9,
            "layout": 8
        },
        {
            "firstKey": 2,
            "secondKey": 10,
            "layout": 8
        },
        {
            "firstKey": 2,
            "secondKey": 11,
            "layout": 8
        },
        {
            "firstKey": 2,
            "secondKey": 12,
            "layout": 8
        },
        {
            "firstKey": 2,
            "secondKey": 13,
            "layout": 9
        },
        {
            "firstKey": 3,
            "secondKey": 0,
            "layout": 10
        },
        {
            "firstKey": 3,
            "secondKey": 1,
            "layout": 10
        },
        {
            "firstKey": 3,
            "secondKey": 2,
            "layout": 6
        },
        {
            "firstKey": 3,
            "secondKey": 4,
            "layout": 8
        },
        {
            "firstKey": 3,
            "secondKey": 5,
            "layout": 8
        },
        {
            "firstKey": 3,
            "secondKey": 6,
            "layout": 8
        },
        {
            "firstKey": 3,
            "secondKey": 7,
            "layout": 8
        },
        {
            "firstKey": 3,
            "secondKey": 8,
            "layout": 8
        },
        {
            "firstKey": 3,
            "secondKey": 9,
            "layout": 8
        },
        {
            "firstKey": 3,
            "secondKey": 10,
            "layout": 8
        },
        {
            "firstKey": 3,
            "secondKey": 11,
            "layout": 8
        },
        {
            "firstKey": 3,
            "secondKey": 12,
            "layout": 8
        },
        {
            "firstKey": 3,
            "secondKey": 13,
            "layout": 9
        },
        {
            "firstKey": 4,
            "secondKey": 0,
            "layout": 11
        },
        {
            "firstKey": 4,
            "secondKey": 1,
            "layout": 11
        },
        {
            "firstKey": 4,
            "secondKey": 2,
            "layout": 9
        },
        {
            "firstKey": 4,
            "secondKey": 3,
            "layout": 9
        },
        {
            "firstKey": 4,
            "secondKey": 5,
            "layout": 3
        },
        {
            "firstKey": 4,
            "secondKey": 6,
            "layout": 6
        },
        {
            "firstKey": 4,
            "secondKey": 7,
            "layout": 9
        },
        {
            "firstKey": 4,
            "secondKey": 8,
            "layout": 9
        },
        {
            "firstKey": 4,
            "secondKey": 9,
            "layout": 9
        },
        {
            "firstKey": 4,
            "secondKey": 10,
            "layout": 9
        },
        {
            "firstKey": 4,
            "secondKey": 11,
            "layout": 9
        },
        {
            "firstKey": 4,
            "secondKey": 12,
            "layout": 9
        },
        {
            "firstKey": 4,
            "secondKey": 13,
            "layout": 10
        },
        {
            "firstKey": 5,
            "secondKey": 0,
            "layout": 11
        },
        {
            "firstKey": 5,
            "secondKey": 1,
            "layout": 11
        },
        {
            "firstKey": 5,
            "secondKey": 2,
            "layout": 9
        },
        {
            "firstKey": 5,
            "secondKey": 3,
            "layout": 9
        },
        {
            "firstKey": 5,
            "secondKey": 4,
            "layout": 3
        },
        {
            "firstKey": 5,
            "secondKey": 6,
            "layout": 6
        },
        {
            "firstKey": 5,
            "secondKey": 7,
            "layout": 9
        },
        {
            "firstKey": 5,
            "secondKey": 8,
            "layout": 9
        },
        {
            "firstKey": 5,
            "secondKey": 9,
            "layout": 9
        },
        {
            "firstKey": 5,
            "secondKey": 10,
            "layout": 9
        },
        {
            "firstKey": 5,
            "secondKey": 11,
            "layout": 9
        },
        {
            "firstKey": 5,
            "secondKey": 12,
            "layout": 9
        },
        {
            "firstKey": 5,
            "secondKey": 13,
            "layout": 10
        },
        {
            "firstKey": 6,
            "secondKey": 0,
            "layout": 13
        },
        {
            "firstKey": 6,
            "secondKey": 1,
            "layout": 13
        },
        {
            "firstKey": 6,
            "secondKey": 2,
            "layout": 11
        },
        {
            "firstKey": 6,
            "secondKey": 3,
            "layout": 11
        },
        {
            "firstKey": 6,
            "secondKey": 4,
            "layout": 8
        },
        {
            "firstKey": 6,
            "secondKey": 5,
            "layout": 8
        },
        {
            "firstKey": 6,
            "secondKey": 7,
            "layout": 11
        },
        {
            "firstKey": 6,
            "secondKey": 8,
            "layout": 11
        },
        {
            "firstKey": 6,
            "secondKey": 9,
            "layout": 11
        },
        {
            "firstKey": 6,
            "secondKey": 10,
            "layout": 11
        },
        {
            "firstKey": 6,
            "secondKey": 11,
            "layout": 11
        },
        {
            "firstKey": 6,
            "secondKey": 12,
            "layout": 11
        },
        {
            "firstKey": 6,
            "secondKey": 13,
            "layout": 12
        },
        {
            "firstKey": 7,
            "secondKey": 0,
            "layout": 14
        },
        {
            "firstKey": 7,
            "secondKey": 1,
            "layout": 14
        },
        {
            "firstKey": 7,
            "secondKey": 2,
            "layout": 12
        },
        {
            "firstKey": 7,
            "secondKey": 3,
            "layout": 12
        },
        {
            "firstKey": 7,
            "secondKey": 4,
            "layout": 12
        },
        {
            "firstKey": 7,
            "secondKey": 5,
            "layout": 12
        },
        {
            "firstKey": 7,
            "secondKey": 6,
            "layout": 12
        },
        {
            "firstKey": 7,
            "secondKey": 8,
            "layout": 12
        },
        {
            "firstKey": 7,
            "secondKey": 9,
            "layout": 12
        },
        {
            "firstKey": 7,
            "secondKey": 10,
            "layout": 12
        },
        {
            "firstKey": 7,
            "secondKey": 11,
            "layout": 12
        },
        {
            "firstKey": 7,
            "secondKey": 12,
            "layout": 12
        },
        {
            "firstKey": 7,
            "secondKey": 13,
            "layout": 13
        },
        {
            "firstKey": 8,
            "secondKey": 0,
            "layout": 8
        },
        {
            "firstKey": 8,
            "secondKey": 1,
            "layout": 8
        },
        {
            "firstKey": 8,
            "secondKey": 2,
            "layout": 6
        },
        {
            "firstKey": 8,
            "secondKey": 3,
            "layout": 6
        },
        {
            "firstKey": 8,
            "secondKey": 4,
            "layout": 6
        },
        {
            "firstKey": 8,
            "secondKey": 5,
            "layout": 6
        },
        {
            "firstKey": 8,
            "secondKey": 6,
            "layout": 6
        },
        {
            "firstKey": 8,
            "secondKey": 7,
            "layout": 6
        },
        {
            "firstKey": 8,
            "secondKey": 9,
            "layout": 5
        },
        {
            "firstKey": 8,
            "secondKey": 10,
            "layout": 5
        },
        {
            "firstKey": 8,
            "secondKey": 11,
            "layout": 6
        },
        {
            "firstKey": 8,
            "secondKey": 12,
            "layout": 6
        },
        {
            "firstKey": 8,
            "secondKey": 13,
            "layout": 7
        },
        {
            "firstKey": 9,
            "secondKey": 0,
            "layout": 7
        },
        {
            "firstKey": 9,
            "secondKey": 1,
            "layout": 7
        },
        {
            "firstKey": 9,
            "secondKey": 2,
            "layout": 5
        },
        {
            "firstKey": 9,
            "secondKey": 3,
            "layout": 5
        },
        {
            "firstKey": 9,
            "secondKey": 4,
            "layout": 5
        },
        {
            "firstKey": 9,
            "secondKey": 5,
            "layout": 5
        },
        {
            "firstKey": 9,
            "secondKey": 6,
            "layout": 5
        },
        {
            "firstKey": 9,
            "secondKey": 7,
            "layout": 5
        },
        {
            "firstKey": 9,
            "secondKey": 8,
            "layout": 4
        },
        {
            "firstKey": 9,
            "secondKey": 10,
            "layout": 2
        },
        {
            "firstKey": 9,
            "secondKey": 11,
            "layout": 5
        },
        {
            "firstKey": 9,
            "secondKey": 12,
            "layout": 5
        },
        {
            "firstKey": 9,
            "secondKey": 13,
            "layout": 6
        },
        {
            "firstKey": 10,
            "secondKey": 0,
            "layout": 9
        },
        {
            "firstKey": 10,
            "secondKey": 1,
            "layout": 9
        },
        {
            "firstKey": 10,
            "secondKey": 2,
            "layout": 7
        },
        {
            "firstKey": 10,
            "secondKey": 3,
            "layout": 7
        },
        {
            "firstKey": 10,
            "secondKey": 4,
            "layout": 7
        },
        {
            "firstKey": 10,
            "secondKey": 5,
            "layout": 7
        },
        {
            "firstKey": 10,
            "secondKey": 6,
            "layout": 7
        },
        {
            "firstKey": 10,
            "secondKey": 7,
            "layout": 7
        },
        {
            "firstKey": 10,
            "secondKey": 8,
            "layout": 6
        },
        {
            "firstKey": 10,
            "secondKey": 9,
            "layout": 4
        },
        {
            "firstKey": 10,
            "secondKey": 11,
            "layout": 7
        },
        {
            "firstKey": 10,
            "secondKey": 12,
            "layout": 7
        },
        {
            "firstKey": 10,
            "secondKey": 13,
            "layout": 8
        },
        {
            "firstKey": 11,
            "secondKey": 0,
            "layout": 7
        },
        {
            "firstKey": 11,
            "secondKey": 1,
            "layout": 7
        },
        {
            "firstKey": 11,
            "secondKey": 2,
            "layout": 5
        },
        {
            "firstKey": 11,
            "secondKey": 3,
            "layout": 5
        },
        {
            "firstKey": 11,
            "secondKey": 4,
            "layout": 5
        },
        {
            "firstKey": 11,
            "secondKey": 5,
            "layout": 5
        },
        {
            "firstKey": 11,
            "secondKey": 6,
            "layout": 5
        },
        {
            "firstKey": 11,
            "secondKey": 7,
            "layout": 5
        },
        {
            "firstKey": 11,
            "secondKey": 8,
            "layout": 5
        },
        {
            "firstKey": 11,
            "secondKey": 9,
            "layout": 5
        },
        {
            "firstKey": 11,
            "secondKey": 10,
            "layout": 5
        },
        {
            "firstKey": 11,
            "secondKey": 12,
            "layout": 4
        },
        {
            "firstKey": 11,
            "secondKey": 13,
            "layout": 6
        },
        {
            "firstKey": 12,
            "secondKey": 0,
            "layout": 7
        },
        {
            "firstKey": 12,
            "secondKey": 1,
            "layout": 7
        },
        {
            "firstKey": 12,
            "secondKey": 2,
            "layout": 5
        },
        {
            "firstKey": 12,
            "secondKey": 3,
            "layout": 5
        },
        {
            "firstKey": 12,
            "secondKey": 4,
            "layout": 5
        },
        {
            "firstKey": 12,
            "secondKey": 5,
            "layout": 5
        },
        {
            "firstKey": 12,
            "secondKey": 6,
            "layout": 5
        },
        {
            "firstKey": 12,
            "secondKey": 7,
            "layout": 5
        },
        {
            "firstKey": 12,
            "secondKey": 8,
            "layout": 5
        },
        {
            "firstKey": 12,
            "secondKey": 9,
            "layout": 5
        },
        {
            "firstKey": 12,
            "secondKey": 10,
            "layout": 5
        },
        {
            "firstKey": 12,
            "secondKey": 11,
            "layout": 4
        },
        {
            "firstKey": 12,
            "secondKey": 13,
            "layout": 6
        },
        {
            "firstKey": 13,
            "secondKey": 0,
            "layout": 6
        },
        {
            "firstKey": 13,
            "secondKey": 1,
            "layout": 6
        },
        {
            "firstKey": 13,
            "secondKey": 2,
            "layout": 5
        },
        {
            "firstKey": 13,
            "secondKey": 3,
            "layout": 5
        },
        {
            "firstKey": 13,
            "secondKey": 4,
            "layout": 5
        },
        {
            "firstKey": 13,
            "secondKey": 5,
            "layout": 5
        },
        {
            "firstKey": 13,
            "secondKey": 6,
            "layout": 5
        },
        {
            "firstKey": 13,
            "secondKey": 7,
            "layout": 5
        },
        {
            "firstKey": 13,
            "secondKey": 8,
            "layout": 5
        },
        {
            "firstKey": 13,
            "secondKey": 9,
            "layout": 5
        },
        {
            "firstKey": 13,
            "secondKey": 10,
            "layout": 5
        },
        {
            "firstKey": 13,
            "secondKey": 11,
            "layout": 5
        },
        {
            "firstKey": 13,
            "secondKey": 12,
            "layout": 5
        }
    ]
}