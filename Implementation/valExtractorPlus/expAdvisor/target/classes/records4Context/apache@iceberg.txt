b4ac277a8f2f1ee32de6846ffed8f7c46bfd09b6###/apache_iceberg/flink/src/main/java/org/apache/iceberg/flink/source/RowDataFileScanTaskReader.java###/RowDataFileScanTaskReader.java###org.apache.iceberg.flink.source.RowDataFileScanTaskReader###open:FileScanTask InputFilesDecryptor ###returndeletes.filter(newIterable(task,deletes.requiredSchema(),idToConstant,inputFilesDecryptor)).iterator();###iterable###deletes.filter(newIterable(task,deletes.requiredSchema(),idToConstant,inputFilesDecryptor))###73:5:75:21
ca468d569231e2cd5d502003586d5a49f53de470###/apache_iceberg/api/src/main/java/org/apache/iceberg/Schema.java###/Schema.java###org.apache.iceberg.Schema###findType:String ###returnfindType(lazyNameToId().get(name));###id###lazyNameToId().get(name)###129:5:129:47
7ced339b0565b26d1d58e0e7d418ca387f9101ac###/apache_iceberg/flink/v1.13/flink/src/test/java/org/apache/iceberg/flink/source/BoundedTableFactory.java###/BoundedTableFactory.java###org.apache.iceberg.flink.source.BoundedTableFactory.BoundedTableSource###getChangelogMode:###returnChangelogMode.newBuilder().addContainedKind(RowKind.INSERT).addContainedKind(RowKind.DELETE).addContainedKind(RowKind.UPDATE_BEFORE).addContainedKind(RowKind.UPDATE_AFTER).build();###builder###ChangelogMode.newBuilder().addContainedKind(RowKind.INSERT)###110:7:115:20
a624f6f247eb7ef5c3d6446ab98d400025d96176###/apache_iceberg/core/src/test/java/org/apache/iceberg/TestSequenceNumberForV2Table.java###/TestSequenceNumberForV2Table.java###org.apache.iceberg.TestSequenceNumberForV2Table###testMergeAppend:###V2Assert.assertEquals("Lastsequencenumbershouldbe1",1,readMetadata().lastSequenceNumber());###base###readMetadata()###46:5:46:103
33b4db66261d68428a3e7ceb55f0b9b6718e133b###/apache_iceberg/api/src/main/java/org/apache/iceberg/PartitionSpec.java###/PartitionSpec.java###org.apache.iceberg.PartitionSpec###lazyFieldsBySourceId:###this.fieldsBySourceId=Multimaps.newListMultimap(Maps.newHashMap(),()->Lists.newArrayListWithCapacity(fields.length));###multiMap###Multimaps.newListMultimap(Maps.newHashMap(),()->Lists.newArrayListWithCapacity(fields.length))###231:7:232:100
4b8e2f4752d46971b78a45db0e5d4b4be0775f26###/apache_iceberg/core/src/test/java/org/apache/iceberg/rest/requests/TestCreateNamespaceRequest.java###/TestCreateNamespaceRequest.java###org.apache.iceberg.rest.requests.TestCreateNamespaceRequest###deserialize:String ###returnmapper().readValue(json,CreateNamespaceRequest.class).validate();###request###mapper().readValue(json,CreateNamespaceRequest.class)###169:5:169:78
e6c08a8628197a67f638937f95eb70a628e0dd9e###/apache_iceberg/flink/v1.14/flink/src/test/java/org/apache/iceberg/flink/source/BoundedTableFactory.java###/BoundedTableFactory.java###org.apache.iceberg.flink.source.BoundedTableFactory.BoundedTableSource###getChangelogMode:###returnChangelogMode.newBuilder().addContainedKind(RowKind.INSERT).addContainedKind(RowKind.DELETE).addContainedKind(RowKind.UPDATE_BEFORE).addContainedKind(RowKind.UPDATE_AFTER).build();###builder###ChangelogMode.newBuilder().addContainedKind(RowKind.INSERT)###110:7:115:20
c9ec76ff5e85e35f220b0ca4dae76ee7187cb614###/apache_iceberg/flink/v1.12/flink/src/test/java/org/apache/iceberg/flink/source/BoundedTableFactory.java###/BoundedTableFactory.java###org.apache.iceberg.flink.source.BoundedTableFactory.BoundedTableSource###getChangelogMode:###returnChangelogMode.newBuilder().addContainedKind(RowKind.INSERT).addContainedKind(RowKind.DELETE).addContainedKind(RowKind.UPDATE_BEFORE).addContainedKind(RowKind.UPDATE_AFTER).build();###builder###ChangelogMode.newBuilder().addContainedKind(RowKind.INSERT)###110:7:115:20
e4df91e87007c2185453a896d5ff9a57b2a9b0c6###/apache_iceberg/spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java###/SparkWrite.java###org.apache.iceberg.spark.source.SparkWrite.DynamicOverwrite###commit:WriterCommitMessage[] ###for(file:files(messages))###files###files(messages)###268:7:271:8
d972f45d93f12d554e1bf50e7b115b3fc8ec3152###/apache_iceberg/spark/src/test/java/com/netflix/iceberg/spark/data/RandomData.java###/RandomData.java###com.netflix.iceberg.spark.data.RandomData.SparkRandomDataGenerator###map:Types.MapType Supplier<Object> Supplier<Object> ###GenericArrayDatakeys=newGenericArrayData(newObject[numEntries]);###keysArr###newObject[numEntries]###230:7:230:76
ffdcf09027e09460b7d7505e65aea119107934a3###/apache_iceberg/spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceBenchmark.java###/IcebergSourceBenchmark.java###org.apache.iceberg.spark.source.IcebergSourceBenchmark###setupSpark:###spark=SparkSession.builder().config("spark.ui.enabled",false).master("local").getOrCreate();###builder###SparkSession.builder().config("spark.ui.enabled",false)###96:5:99:24
28e7a1b72cd28f3c7dd1fd2704bcab6963466be2###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java###/RowDataRewriter.java###org.apache.iceberg.spark.source.RowDataRewriter###rewriteDataForTask:CombinedScanTask ###SparkAppenderFactoryappenderFactory=newSparkAppenderFactory(properties,schema,SparkSchemaUtil.convert(schema));###structType###SparkSchemaUtil.convert(schema)###95:5:96:62
8e026f140b4e5d213083e57321d8bdc312650a6f###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueWriters.java###/SparkOrcValueWriters.java###org.apache.iceberg.spark.data.SparkOrcValueWriters.DoubleWriter###nonNullWrite:int int SpecializedGetters ColumnVector ###((DoubleColumnVector)output).vector[rowId]=data.getDouble(column);###doubleValue###data.getDouble(column)###154:7:154:76
a81d11e78fb447f68b2a3e8f98cf02b0c57c7f58###/apache_iceberg/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java###/TestRewriteManifests.java###org.apache.iceberg.TestRewriteManifests###testInvalidUsage:###ManifestFileinvalidAddedFileManifest=writeManifest("manifest-file-2.avro",manifestEntry(ManifestEntry.Status.ADDED,snapshot.snapshotId(),FILE_A));###appendEntry###manifestEntry(ManifestEntry.Status.ADDED,snapshot.snapshotId(),FILE_A)###993:5:995:83
30711e2a744aa4e154c4e4c55a51fca621126363###/apache_iceberg/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java###/MergingSnapshotProducer.java###org.apache.iceberg.MergingSnapshotProducer###copyManifest:ManifestFile ###returnManifestFiles.copyAppendManifest(ops.current().formatVersion(),reader,newManifestPath,snapshotId(),appendedManifestsSummary);###current###ops.current()###232:7:233:107
5ed15d061d07874c8d11bcbad269e4b67e26d83d###/apache_iceberg/mr/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimeObjectInspector.java###/TestIcebergTimeObjectInspector.java###org.apache.iceberg.mr.hive.serde.objectinspector.TestIcebergTimeObjectInspector###testIcebergTimeObjectInspector:###Stringtime=LocalTime.now().toString();###localTime###LocalTime.now()###51:5:51:46
36e261bb40a0502185621165e0fa3441d4273d11###/apache_iceberg/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java###/VectorizedArrowReader.java###org.apache.iceberg.arrow.vectorized.VectorizedArrowReader.PositionVectorReader###read:VectorHolder int ###BitVectorHelper.setBit(vec.getValidityBuffer(),i);###validityBuffer###vec.getValidityBuffer()###427:11:427:62
ec69a25da24f38419107bfa264c582d16331c7e9###/apache_iceberg/data/src/main/java/org/apache/iceberg/data/InternalRecordWrapper.java###/InternalRecordWrapper.java###org.apache.iceberg.data.InternalRecordWrapper###get:int Class<T> ###returnjavaClass.cast(transforms[pos].apply(wrapped.get(pos,Object.class)));###value###wrapped.get(pos,Object.class)###84:7:84:84
4b8e2f4752d46971b78a45db0e5d4b4be0775f26###/apache_iceberg/core/src/test/java/org/apache/iceberg/rest/requests/TestUpdateNamespacePropertiesRequest.java###/TestUpdateNamespacePropertiesRequest.java###org.apache.iceberg.rest.requests.TestUpdateNamespacePropertiesRequest###deserialize:String ###returnmapper().readValue(json,UpdateNamespacePropertiesRequest.class).validate();###request###mapper().readValue(json,UpdateNamespacePropertiesRequest.class)###229:5:229:88
bdd7fa470434d484ba5c8eb27a73055976be2804###/apache_iceberg/spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceFlatDataBenchmark.java###/IcebergSourceFlatDataBenchmark.java###org.apache.iceberg.spark.source.IcebergSourceFlatDataBenchmark###initTable:###returntables.create(schema,partitionSpec,Maps.newHashMap(),newTableLocation());###properties###Maps.newHashMap()###56:5:56:88
704cd8c2e4631798dfa913bec5c06bc4231cfd26###/apache_iceberg/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java###/ParquetUtil.java###org.apache.iceberg.parquet.ParquetUtil###updateMax:Map<Integer,Literal<?>> int Type Literal<T> MetricsMode ###upperBounds.put(id,UnicodeUtil.truncateStringMax((Literal<CharSequence>)max,truncateLength));###truncatedMaxString###UnicodeUtil.truncateStringMax((Literal<CharSequence>)max,truncateLength)###289:13:289:109
4b8e2f4752d46971b78a45db0e5d4b4be0775f26###/apache_iceberg/core/src/test/java/org/apache/iceberg/rest/responses/TestRESTCatalogConfigResponse.java###/TestRESTCatalogConfigResponse.java###org.apache.iceberg.rest.responses.TestRESTCatalogConfigResponse###deserialize:String ###returnmapper().readValue(json,RESTCatalogConfigResponse.class).validate();###resp###mapper().readValue(json,RESTCatalogConfigResponse.class)###271:5:271:81
a81d11e78fb447f68b2a3e8f98cf02b0c57c7f58###/apache_iceberg/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java###/TestRewriteManifests.java###org.apache.iceberg.TestRewriteManifests###testManifestReplacementCombinedWithRewriteConcurrentDelete:###ManifestFilenewManifest=writeManifest("manifest-file-1.avro",manifestEntry(ManifestEntry.Status.EXISTING,firstSnapshot.snapshotId(),FILE_A));###entry###manifestEntry(ManifestEntry.Status.EXISTING,firstSnapshot.snapshotId(),FILE_A)###944:5:946:91
3131b2dcc6786023f26a5785160bd1ad546b97a8###/apache_iceberg/core/src/main/java/org/apache/iceberg/ManifestGroup.java###/ManifestGroup.java###org.apache.iceberg.ManifestGroup###planFiles:###PartitionSpecspec=specsById.get(manifest.partitionSpecId());###partitionSpecId###manifest.partitionSpecId()###138:7:138:70
324b11a02b8b3062cf555e0eb97bbe93ece57e31###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java###/Catalogs.java###org.apache.iceberg.mr.Catalogs###addCatalogPropertiesIfMissing:Configuration String Map<String,String> ###catalogProperties.putIfAbsent(CatalogProperties.WAREHOUSE_LOCATION,conf.get(InputFormatConfig.HADOOP_CATALOG_WAREHOUSE_LOCATION));###legacyWarehouseLocation###conf.get(InputFormatConfig.HADOOP_CATALOG_WAREHOUSE_LOCATION)###238:7:239:78
65319e911235a8cfed6f616955f1f92192d8a52f###/apache_iceberg/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergOutputCommitter.java###/TestHiveIcebergOutputCommitter.java###org.apache.iceberg.mr.hive.TestHiveIcebergOutputCommitter###writeRecords:String int int boolean boolean JobConf OutputCommitter ###HiveIcebergRecordWritertestWriter=newHiveIcebergRecordWriter(schema,spec,FileFormat.PARQUET,newGenericAppenderFactory(schema),outputFileFactory,io,TARGET_FILE_SIZE,TezUtil.taskAttemptWrapper(taskId),conf.get(Catalogs.NAME));###fileFormat###FileFormat.PARQUET###280:7:282:72
83ee9f4f568cc19cf2d5b1f7c0f8133d1a44f5ab###/apache_iceberg/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java###/MergingSnapshotProducer.java###org.apache.iceberg.MergingSnapshotProducer###add:ManifestFile ###appendManifests.add(ManifestWriter.copyAppendManifest(reader,manifestPath(manifestCount.getAndIncrement()),snapshotId(),appendedManifestsSummary));###manifestFile###ManifestWriter.copyAppendManifest(reader,manifestPath(manifestCount.getAndIncrement()),snapshotId(),appendedManifestsSummary)###208:7:209:107
ba655b7180dbfc89783f35ba13ee771ce9050598###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java###/SparkTableUtil.java###org.apache.iceberg.spark.SparkTableUtil###importSparkPartitions:SparkSession List<SparkPartition> Table PartitionSpec String ###List<ManifestFile>manifests=partitionDS.flatMap((FlatMapFunction<SparkPartition,DataFile>)sparkPartition->listPartition(sparkPartition,spec,serializableConf,metricsConfig,nameMapping).iterator(),Encoders.javaSerialization(DataFile.class)).repartition(numShufflePartitions).map((MapFunction<DataFile,Tuple2<String,DataFile>>)file->Tuple2.apply(file.path().toString(),file),Encoders.tuple(Encoders.STRING(),Encoders.javaSerialization(DataFile.class))).orderBy(col("_1")).mapPartitions((MapPartitionsFunction<Tuple2<String,DataFile>,ManifestFile>)fileTuple->buildManifest(serializableConf,spec,stagingDir,fileTuple),Encoders.javaSerialization(ManifestFile.class)).collectAsList();###filesToImport###partitionDS.flatMap((FlatMapFunction<SparkPartition,DataFile>)sparkPartition->listPartition(sparkPartition,spec,serializableConf,metricsConfig,nameMapping).iterator(),Encoders.javaSerialization(DataFile.class))###472:5:485:26
5e3f9198e5675a852df4f0e1c28b4e3cf6630f86###/apache_iceberg/aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java###/S3OutputStream.java###org.apache.iceberg.aws.s3.S3OutputStream###write:byte[] int int ###stream.write(b,off,len);###remaining###len###87:5:87:31
0f2a164e6ac711c2f2900c91d86c7ab338ac64ee###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java###/HiveIcebergStorageHandler.java###org.apache.iceberg.mr.hive.HiveIcebergStorageHandler###put:Configuration Table ###config.set(InputFormatConfig.ENCRYPTION_MANAGER,SerializationUtil.serializeToBase64(table.encryption()));###base64EncryptionManager###SerializationUtil.serializeToBase64(table.encryption())###211:5:211:111
65319e911235a8cfed6f616955f1f92192d8a52f###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputFormat.java###/HiveIcebergOutputFormat.java###org.apache.iceberg.mr.hive.HiveIcebergOutputFormat###writer:JobConf ###OutputFileFactoryoutputFileFactory=newOutputFileFactory(spec,fileFormat,location,io,encryption,taskAttemptID.getTaskID().getId(),taskAttemptID.getId(),jc.get(HiveConf.ConfVars.HIVEQUERYID.varname)+"-"+taskAttemptID.getJobID());###partitionId###taskAttemptID.getTaskID().getId()###83:5:85:116
5a3cd22e775dfa8bf79deab675390aad48ba79a5###/apache_iceberg/parquet/src/main/java/org/apache/iceberg/parquet/PruneColumns.java###/PruneColumns.java###org.apache.iceberg.parquet.PruneColumns###message:MessageType List<Type> ###if(selectedIds.contains(getId(originalField)))###fieldId###getId(originalField)###48:7:55:8
7fd7ccbcc5493b98aba3d5a26117e87d299bdbcf###/apache_iceberg/aws/src/test/java/org/apache/iceberg/aws/s3/TestS3OutputStream.java###/TestS3OutputStream.java###org.apache.iceberg.aws.s3.TestS3OutputStream###testWrite:###writeAndVerify(s3mock,randomURI(),randomData(1024),arrayWrite);###data###randomData(1024)###104:7:104:73
c5e67915b01c2cbf2eea40a412c2765f0c8278e8###/apache_iceberg/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java###/TestFlinkCatalogTable.java###org.apache.iceberg.flink.TestFlinkCatalogTable###testRenameTable:###validationCatalog.createTable(TableIdentifier.of(icebergNamespace,"tl"),newSchema(Types.NestedField.optional(0,"id",Types.LongType.get())));###tableSchema###newSchema(Types.NestedField.optional(0,"id",Types.LongType.get()))###99:5:101:80
85e632b838f569eb5bb13f966ec2c98b74ebf728###/apache_iceberg/core/src/main/java/com/netflix/iceberg/GenericDataFile.java###/GenericDataFile.java###com.netflix.iceberg.GenericDataFile###GenericDataFile:Schema ###this.partitionType=schema.fieldType("partition").asNestedType().asStructType();###partType###schema.fieldType("partition")###68:5:68:86
bf9a22738b1160749b113f9b5f090a5e8d4c3827###/apache_iceberg/api/src/main/java/org/apache/iceberg/transforms/Truncate.java###/Truncate.java###org.apache.iceberg.transforms.Truncate.TruncateString###project:String BoundPredicate<CharSequence> ###returnProjectionUtil.truncateArray(name,predicate.asLiteralPredicate(),this);###pred###predicate.asLiteralPredicate()###288:9:288:89
dc13c05f5355367e25d159334fea07a1f76540fa###/apache_iceberg/core/src/main/java/org/apache/iceberg/PartitionsTable.java###/PartitionsTable.java###org.apache.iceberg.PartitionsTable###task:TableScan ###returnStaticDataTask.of(ops.io().newInputFile(ops.current().metadataFileLocation()),partitions(table,scan.snapshot().snapshotId()),PartitionsTable::convertPartition);###partitions###partitions(table,scan.snapshot().snapshotId())###67:5:70:44
b01506b4321d869e728ea44b2424b0edba898f9d###/apache_iceberg/spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/source/SparkMergeScan.java###/SparkMergeScan.java###org.apache.iceberg.spark.source.SparkMergeScan###filterFiles:Set<String> ###files=files().stream().filter(file->filteredLocations.contains(file.file().path().toString())).collect(Collectors.toList());###originalFile###files()###100:5:102:39
b655d40e8e6956640163d30700c8f20e4d571fbf###/apache_iceberg/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java###/TestIcebergSourceTablesBase.java###org.apache.iceberg.spark.source.TestIcebergSourceTablesBase###testEntriesTable:###Assert.assertEquals("Shouldonlycontainonemanifest",1,table.currentSnapshot().manifests().size());###snapshot###table.currentSnapshot()###119:5:119:108
649cbdde83693ebda8e8dc6e75857426d25414ec###/apache_iceberg/api/src/main/java/org/apache/iceberg/PartitionSpec.java###/PartitionSpec.java###org.apache.iceberg.PartitionSpec.Builder###hour:String ###fields.add(newPartitionField(sourceColumn.fieldId(),name,Transforms.hour(sourceColumn.type())));###field###newPartitionField(sourceColumn.fieldId(),name,Transforms.hour(sourceColumn.type()))###365:7:366:80
56ae3748945add6044671ddd30cde6a815c6ddb4###/apache_iceberg/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java###/FlinkSink.java###org.apache.iceberg.flink.sink.FlinkSink.Builder###build:###DataStream<Void>returnStream=rowDataInput.transform(ICEBERG_STREAM_WRITER_NAME,TypeInformation.of(WriteResult.class),streamWriter).setParallelism(writeParallelism).transform(ICEBERG_FILES_COMMITTER_NAME,Types.VOID,filesCommitter).setParallelism(1).setMaxParallelism(1);###writerStream###rowDataInput.transform(ICEBERG_STREAM_WRITER_NAME,TypeInformation.of(WriteResult.class),streamWriter).setParallelism(writeParallelism)###246:7:251:33
9ed321672cb8518616194fe6eae18bdfee5247af###/apache_iceberg/mr/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergBinaryObjectInspector.java###/TestIcebergBinaryObjectInspector.java###org.apache.iceberg.mr.hive.serde.objectinspector.TestIcebergBinaryObjectInspector###testIcebergByteArrayObjectInspector:###Assert.assertEquals(newBytesWritable(bytes),oi.getPrimitiveWritableObject(bytes));###bytesWritable###newBytesWritable(bytes)###53:5:53:89
33f1825b72efe05c7d83feb68fa1669eb00bb129###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java###/RowDataReader.java###org.apache.iceberg.spark.source.RowDataReader###newAvroIterable:InputFile FileScanTask Schema Map<Integer,?> ###returnAvro.read(location).reuseContainers().project(projection).split(task.start(),task.length()).createReaderFunc(readSchema->newSparkAvroReader(projection,readSchema,idToConstant)).build();###builder###Avro.read(location).reuseContainers().project(projection).split(task.start(),task.length()).createReaderFunc(readSchema->newSparkAvroReader(projection,readSchema,idToConstant))###144:5:149:18
8a84715205631a17668ec8d63a791a25fb511992###/apache_iceberg/core/src/test/java/org/apache/iceberg/TestManifestWriter.java###/TestManifestWriter.java###org.apache.iceberg.TestManifestWriter###newFile:long ###returnDataFiles.builder(SPEC).withPath("data_bucket=0/"+fileName+".parquet").withFileSizeInBytes(1024).withRecordCount(recordCount).build();###builder###DataFiles.builder(SPEC).withPath("data_bucket=0/"+fileName+".parquet").withFileSizeInBytes(1024).withRecordCount(recordCount)###70:5:74:18
13ee93a5e33a252cfe7aa8c2c4a51060c2c42238###/apache_iceberg/spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java###/SchemaEvolutionTest.java###org.apache.iceberg.examples.SchemaEvolutionTest###addColumnToSchema:###table.updateSchema().addColumn("publisher",Types.StringType.get()).commit();###fieldName###"publisher"###87:5:87:82
01d1462756db20a14a9ac67166e5bf56966861b4###/apache_iceberg/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java###/TestIcebergSourceTablesBase.java###org.apache.iceberg.spark.source.TestIcebergSourceTablesBase###testAllDataFilesTable:###expected.add((GenericData.Record)record.get("data_file"));###file###(GenericData.Record)record.get("data_file")###469:13:469:72
d972f45d93f12d554e1bf50e7b115b3fc8ec3152###/apache_iceberg/spark/src/test/java/com/netflix/iceberg/spark/data/RandomData.java###/RandomData.java###com.netflix.iceberg.spark.data.RandomData.SparkRandomDataGenerator###list:Types.ListType Supplier<Object> ###GenericArrayDataresult=newGenericArrayData(newObject[numElements]);###arr###newObject[numElements]###212:7:212:79
d3a6107df2f692a056b681c4087bc59398e26e06###/apache_iceberg/mr/src/test/java/org/apache/iceberg/mr/TestCatalogs.java###/TestCatalogs.java###org.apache.iceberg.mr.TestCatalogs###testLoadCatalogDefault:###Optional<Catalog>defaultCatalog=Catalogs.loadCatalog(conf,"barCatalog");###catalogName###"barCatalog"###244:5:244:81
80cbc60ee55911ee627a7ad3013804394d7b5e9a###/apache_iceberg/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java###/RemoveSnapshots.java###org.apache.iceberg.RemoveSnapshots###cleanExpiredFiles:List<Snapshot> Set<Long> Set<Long> ###booleanfromValidSnapshots=validIds.contains(manifest.snapshotId());###snapshotId###manifest.snapshotId()###198:11:198:81
c53d4bee440ed8671b68a74f7c71662d14561359###/apache_iceberg/api/src/main/java/org/apache/iceberg/types/Conversions.java###/Conversions.java###org.apache.iceberg.types.Conversions###internalFromByteBuffer:Type ByteBuffer ###ByteBuffertmp=buffer.duplicate().order(ByteOrder.LITTLE_ENDIAN);###tmp###buffer.duplicate()###120:5:120:72
e4df91e87007c2185453a896d5ff9a57b2a9b0c6###/apache_iceberg/spark2/src/main/java/org/apache/iceberg/spark/source/Writer.java###/Writer.java###org.apache.iceberg.spark.source.Writer###replacePartitions:WriterCommitMessage[] ###for(file:files(messages))###files###files(messages)###190:5:193:6
c5eeb97085f22927d87d1ef1d4791256c0eb7ad5###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java###/Catalogs.java###org.apache.iceberg.mr.Catalogs###hiveCatalog:Configuration Properties ###returnCatalogUtil.ICEBERG_CATALOG_TYPE_HIVE.equalsIgnoreCase(getCatalogType(conf,catalogName));###catalogType###getCatalogType(conf,catalogName)###205:5:205:102
0f2a164e6ac711c2f2900c91d86c7ab338ac64ee###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java###/HiveIcebergStorageHandler.java###org.apache.iceberg.mr.hive.HiveIcebergStorageHandler###put:Configuration Table ###config.set(InputFormatConfig.LOCATION_PROVIDER,SerializationUtil.serializeToBase64(table.locationProvider()));###base64LocationProvider###SerializationUtil.serializeToBase64(table.locationProvider())###210:5:210:116
649cbdde83693ebda8e8dc6e75857426d25414ec###/apache_iceberg/api/src/main/java/org/apache/iceberg/PartitionSpec.java###/PartitionSpec.java###org.apache.iceberg.PartitionSpec.Builder###day:String ###fields.add(newPartitionField(sourceColumn.fieldId(),name,Transforms.day(sourceColumn.type())));###field###newPartitionField(sourceColumn.fieldId(),name,Transforms.day(sourceColumn.type()))###356:7:357:79
a90c7acca6cc8aec08362b8c193f89b980a8c63d###/apache_iceberg/spark/src/main/java/com/netflix/iceberg/spark/source/Reader.java###/Reader.java###com.netflix.iceberg.spark.source.Reader.PartitionRowConverter###apply:StructLike ###reusedRow.update(i,convert(tuple.get(positions[i],javaTypes[i]),types[i]));###value###tuple.get(positions[i],javaTypes[i])###491:9:491:87
f258bfa1fa89239f0dae18ed49f911602954de1b###/apache_iceberg/spark/src/main/java/com/netflix/iceberg/spark/SparkExpressions.java###/SparkExpressions.java###com.netflix.iceberg.spark.SparkExpressions###convert:Expression ###returnnot(convert(((Not)expr).child()));###child###convert(((Not)expr).child())###126:11:126:53
4b8e2f4752d46971b78a45db0e5d4b4be0775f26###/apache_iceberg/core/src/test/java/org/apache/iceberg/rest/responses/TestCreateNamespaceResponse.java###/TestCreateNamespaceResponse.java###org.apache.iceberg.rest.responses.TestCreateNamespaceResponse###deserialize:String ###returnmapper().readValue(json,CreateNamespaceResponse.class).validate();###response###mapper().readValue(json,CreateNamespaceResponse.class)###172:5:172:79
4b8e2f4752d46971b78a45db0e5d4b4be0775f26###/apache_iceberg/core/src/test/java/org/apache/iceberg/rest/responses/TestListTablesResponse.java###/TestListTablesResponse.java###org.apache.iceberg.rest.responses.TestListTablesResponse###deserialize:String ###returnmapper().readValue(json,ListTablesResponse.class).validate();###resp###mapper().readValue(json,ListTablesResponse.class)###149:5:149:74
65319e911235a8cfed6f616955f1f92192d8a52f###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputFormat.java###/HiveIcebergOutputFormat.java###org.apache.iceberg.mr.hive.HiveIcebergOutputFormat###writer:JobConf ###OutputFileFactoryoutputFileFactory=newOutputFileFactory(spec,fileFormat,location,io,encryption,taskAttemptID.getTaskID().getId(),taskAttemptID.getId(),jc.get(HiveConf.ConfVars.HIVEQUERYID.varname)+"-"+taskAttemptID.getJobID());###operationId###jc.get(HiveConf.ConfVars.HIVEQUERYID.varname)+"-"+taskAttemptID.getJobID()###83:5:85:116
2e4684740b56f6d80328f50b7e7fef0863c262e7###/apache_iceberg/data/src/test/java/org/apache/iceberg/TestMergingMetrics.java###/TestMergingMetrics.java###org.apache.iceberg.TestMergingMetrics###verifySingleRecordMetric:###Map<Integer,Long>nanValueCount=appender.metrics().nanValueCounts();###metrics###appender.metrics()###117:5:117:76
6048e5a794242cb83871e3838d0d40aa71e36a91###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/source/Writer.java###/Writer.java###org.apache.iceberg.spark.source.Writer.WriterFactory.OutputFileFactory###newOutputFile:PartitionKey ###OutputFilerawOutputFile=fileIo.newOutputFile(locations.newDataLocation(spec,key,generateFilename()));###newDataLocation###locations.newDataLocation(spec,key,generateFilename())###349:9:349:115
aba898b1a2ea15fd091228626b6887a5a72800c0###/apache_iceberg/core/src/test/java/org/apache/iceberg/TestDeleteFileIndex.java###/TestDeleteFileIndex.java###org.apache.iceberg.TestDeleteFileIndex###testUnpartitionedDeletes:###DeleteFileIndexindex=newDeleteFileIndex(ImmutableMap.of(PartitionSpec.unpartitioned().specId(),PartitionSpec.unpartitioned(),1,SPEC),newlong[]{3,5,5,6},DELETE_FILES,ImmutableMap.of());###partSpec###PartitionSpec.unpartitioned()###81:5:85:69
d1510340eaff68d88a2e8194d58e7e493af02bcc###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java###/HiveIcebergOutputCommitter.java###org.apache.iceberg.mr.hive.HiveIcebergOutputCommitter###cleanup:JobContext ###FileSystemfs=Util.getFs(toDelete,jobContext.getJobConf());###jobConf###jobContext.getJobConf()###210:11:210:73
60645579c4c4e6c8de89758ee22034667c4a8e48###/apache_iceberg/core/src/main/java/com/netflix/iceberg/GenericDataFile.java###/GenericDataFile.java###com.netflix.iceberg.GenericDataFile###get:int ###thrownewUnsupportedOperationException("Unknownfieldordinal:"+i);###pos###i###320:9:320:80
1e39b4159a968017142076a38e3d866df1d8c073###/apache_iceberg/core/src/main/java/com/netflix/iceberg/TableMetadata.java###/TableMetadata.java###com.netflix.iceberg.TableMetadata###rollbackTo:Snapshot ###returnnewTableMetadata(ops,null,location,System.currentTimeMillis(),lastColumnId,schema,spec,properties,snapshot.snapshotId(),snapshots);###nowMillis###System.currentTimeMillis()###206:5:208:20
78495a20bfc207e4f9c3f6cae2b9be14923f3562###/apache_iceberg/parquet/src/test/java/org/apache/iceberg/parquet/ParquetWritingTestUtils.java###/ParquetWritingTestUtils.java###org.apache.iceberg.parquet.ParquetWritingTestUtils###writeRecords:TemporaryFolder Schema Map<String,String> Function<MessageType,ParquetValueWriter<?>> GenericData.Record[] ###FileAppender<GenericData.Record>writer=Parquet.write(localOutput(file)).schema(schema).setAll(properties).createWriterFunc(createWriterFunc).build()###writer###Parquet.write(localOutput(file)).schema(schema).setAll(properties).createWriterFunc(createWriterFunc).build()###63:10:67:17
7ecf6724a9a01462729e667b47e623b625a6caae###/apache_iceberg/data/src/main/java/org/apache/iceberg/data/parquet/BaseParquetReaders.java###/BaseParquetReaders.java###org.apache.iceberg.data.parquet.BaseParquetReaders.ReadBuilder###struct:Types.StructType GroupType List<ParquetValueReader<?>> ###readersById.put(id,ParquetValueReaders.option(fieldType,fieldD,fieldReaders.get(i)));###fieldReader###fieldReaders.get(i)###133:9:133:97
01d1462756db20a14a9ac67166e5bf56966861b4###/apache_iceberg/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java###/TestIcebergSourceTablesBase.java###org.apache.iceberg.spark.source.TestIcebergSourceTablesBase###testFilesTable:###expected.add((GenericData.Record)record.get("data_file"));###file###(GenericData.Record)record.get("data_file")###225:13:225:72
2e4436b0a34deea1d20c2ca7f9b6bfc94d5482f2###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java###/StructInternalRow.java###org.apache.iceberg.spark.source.StructInternalRow###fillArray:Collection<?> Function<ArrayData,BiConsumer<Integer,T>> ###GenericArrayDataarray=newGenericArrayData(newObject[values.size()]);###array###newObject[values.size()]###260:5:260:78
547e4e1c954179e20ec4ebab3896f2b1d927c168###/apache_iceberg/core/src/main/java/org/apache/iceberg/FastAppend.java###/FastAppend.java###org.apache.iceberg.FastAppend###updateEvent:###longsequenceNumber=ops.current().snapshot(snapshotId).sequenceNumber();###snapshot###ops.current().snapshot(snapshotId)###153:5:153:79
543a6cdd0538e505c4ab3fa0f0876a5a22bd7704###/apache_iceberg/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java###/FlinkTestBase.java###org.apache.iceberg.flink.FlinkTestBase###getTableEnv:###this.tEnv=TableEnvironment.create(EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build());###settings###EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build()###65:11:68:39
0f9370b5d0346826296e3cbb6ada591760fd4cf5###/apache_iceberg/api/src/main/java/com/netflix/iceberg/types/ReassignIds.java###/ReassignIds.java###com.netflix.iceberg.types.ReassignIds###map:Types.MapType Supplier<Type> Supplier<Type> ###returnTypes.MapType.ofOptional(sourceKeyId,sourceValueId,keyTypeFuture.get(),valueTypeFuture.get());###keyType###keyTypeFuture.get()###113:9:114:57
5a3cd22e775dfa8bf79deab675390aad48ba79a5###/apache_iceberg/parquet/src/main/java/org/apache/iceberg/parquet/PruneColumns.java###/PruneColumns.java###org.apache.iceberg.parquet.PruneColumns###struct:GroupType List<Type> ###if(selectedIds.contains(getId(originalField)))###fieldId###getId(originalField)###74:7:79:8
ae80264a5472d7190da4c6368e71e7e368228e77###/apache_iceberg/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java###/MergingSnapshotProducer.java###org.apache.iceberg.MergingSnapshotProducer###validationHistory:TableMetadata Long Set<String> ManifestContent ###ValidationException.check(currentSnapshot!=null,"Cannotdeterminehistorybetweenstartingsnapshot%sandcurrent%s",startingSnapshotId,currentSnapshotId);###lastSnapshot###currentSnapshot###470:7:472:50
649cbdde83693ebda8e8dc6e75857426d25414ec###/apache_iceberg/api/src/main/java/org/apache/iceberg/PartitionSpec.java###/PartitionSpec.java###org.apache.iceberg.PartitionSpec.Builder###year:String ###fields.add(newPartitionField(sourceColumn.fieldId(),name,Transforms.year(sourceColumn.type())));###field###newPartitionField(sourceColumn.fieldId(),name,Transforms.year(sourceColumn.type()))###338:7:339:80
fa17d8259d5f7304be197636b9d8ec814f2c5c07###/apache_iceberg/flink/src/main/java/org/apache/iceberg/flink/FlinkDynamicTableFactory.java###/FlinkDynamicTableFactory.java###org.apache.iceberg.flink.FlinkDynamicTableFactory###createDynamicTableSource:Context ###returnnewIcebergTableSource(tableLoader,tableSchema,context.getCatalogTable().getOptions(),context.getConfiguration());###tableProps###context.getCatalogTable().getOptions()###44:5:45:37
06362f3af2e6493bab458693b28d2a7e50899014###/apache_iceberg/core/src/main/java/org/apache/iceberg/SchemaUpdate.java###/SchemaUpdate.java###org.apache.iceberg.SchemaUpdate###applyChangesToMapping:TableMetadata ###returnmetadata;###newMetadata###metadata###422:5:422:21
06b5f045381d5e4693bfe4a374ce656f6c8ae836###/apache_iceberg/flink/src/test/java/org/apache/iceberg/flink/data/TestFlinkAvroReaderWriter.java###/TestFlinkAvroReaderWriter.java###org.apache.iceberg.flink.data.TestFlinkAvroReaderWriter###writeAndValidate:Schema ###newFlinkAvroWriter(FlinkSchemaUtil.convert(schema))###flinkSchema###FlinkSchemaUtil.convert(schema)###53:37:53:89
6d3b1f7996d4ceee5d5e28df7f378f6fe377950e###/apache_iceberg/core/src/main/java/org/apache/iceberg/TableMetadata.java###/TableMetadata.java###org.apache.iceberg.TableMetadata###freshSortOrder:int Schema SortOrder ###SortOrder.Builderbuilder=SortOrder.builderFor(schema).withOrderId(orderId);###builder###SortOrder.builderFor(schema)###998:5:998:83
0d68114344ec127c9438425407149f900cedc121###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java###/HiveIcebergStorageHandler.java###org.apache.iceberg.mr.hive.HiveIcebergStorageHandler###configureInputJobProperties:TableDesc Map<String,String> ###map.put(InputFormatConfig.TABLE_SCHEMA,SchemaParser.toJson(table.schema()));###schemaJson###SchemaParser.toJson(table.schema())###79:5:79:82
5d781f19ad0bfdba2523c8483d51bf3cf31c7d81###/apache_iceberg/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java###/MergingSnapshotUpdate.java###org.apache.iceberg.MergingSnapshotUpdate###apply:TableMetadata ###for(manifest:mergeGroup(entry.getKey(),entry.getValue()))###groupId###entry.getKey()###251:9:253:10
8e026f140b4e5d213083e57321d8bdc312650a6f###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueWriters.java###/SparkOrcValueWriters.java###org.apache.iceberg.spark.data.SparkOrcValueWriters.FloatWriter###nonNullWrite:int int SpecializedGetters ColumnVector ###((DoubleColumnVector)output).vector[rowId]=data.getFloat(column);###floatValue###data.getFloat(column)###145:7:145:75
9f1598e7f9c3409e2ce38365f928ad6d5d58e485###/apache_iceberg/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java###/PruneColumns.java###org.apache.iceberg.avro.PruneColumns###array:Schema Schema ###returnAvroSchemaUtil.createMap(keyId,keyValue.getField("key").schema(),valueId,element.getField("value").schema());###valueProjection###element.getField("value").schema()###115:11:117:60
65319e911235a8cfed6f616955f1f92192d8a52f###/apache_iceberg/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergOutputCommitter.java###/TestHiveIcebergOutputCommitter.java###org.apache.iceberg.mr.hive.TestHiveIcebergOutputCommitter###writeRecords:String int int boolean boolean JobConf OutputCommitter ###OutputFileFactoryoutputFileFactory=newOutputFileFactory(spec,FileFormat.PARQUET,location,io,encryption,taskId.getTaskID().getId(),attemptNum,QUERY_ID+"-"+JOB_ID);###operationId###QUERY_ID+"-"+JOB_ID###277:7:279:52
c2957eb45cc4ee0756d11ff9a9567d3f1cca0ba4###/apache_iceberg/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java###/ParquetUtil.java###org.apache.iceberg.parquet.ParquetUtil###footerMetrics:ParquetMetadata int ###updateMin(lowerBounds,fieldId,field.type(),fromParquetPrimitive(field.type(),column.getPrimitiveType(),stats.genericGetMin()),statsTruncateLength);###min###fromParquetPrimitive(field.type(),column.getPrimitiveType(),stats.genericGetMin())###104:13:106:70
c383dd87a89e35d622e9c458fd711931cbc5e96f###/apache_iceberg/parquet/src/main/java/com/netflix/iceberg/parquet/ParquetMetrics.java###/ParquetMetrics.java###com.netflix.iceberg.parquet.ParquetMetrics###fromMetadata:ParquetMetadata ###intfieldId=fileSchema.aliasToId(column.getPath().toDotString());###path###column.getPath()###74:9:74:76
1a94be338e48b3a8516ebd7f68871b6588f7ec27###/apache_iceberg/nessie/src/main/java/org/apache/iceberg/nessie/NessieTableOperations.java###/NessieTableOperations.java###org.apache.iceberg.nessie.NessieTableOperations###doCommit:TableMetadata TableMetadata ###Operationsop=ImmutableOperations.builder().addOperations(Operation.Put.of(key,newTable.build())).commitMeta(NessieUtil.buildCommitMetadata("icebergcommit",catalogOptions)).build();###icebergTable###newTable.build()###112:7:113:97
d972f45d93f12d554e1bf50e7b115b3fc8ec3152###/apache_iceberg/spark/src/test/java/com/netflix/iceberg/spark/data/RandomData.java###/RandomData.java###com.netflix.iceberg.spark.data.RandomData.SparkRandomDataGenerator###map:Types.MapType Supplier<Object> Supplier<Object> ###GenericArrayDatakeys=newGenericArrayData(newObject[numEntries]);###valuesArr###newObject[numEntries]###230:7:230:76
3287fee28359d303df28fef6fc9ea7b95cfa348f###/apache_iceberg/api/src/main/java/org/apache/iceberg/types/Comparators.java###/Comparators.java###org.apache.iceberg.types.Comparators.CharSeqComparator###compare:CharSequence CharSequence ###intcmp=Character.compare(s1.charAt(i),s2.charAt(i));###c2###s2.charAt(i)###191:9:191:65
ae96c325239505bccde5eaeec42dbdde4df5fa6a###/apache_iceberg/api/src/main/java/org/apache/iceberg/transforms/Dates.java###/Dates.java###org.apache.iceberg.transforms.Dates###apply:Integer ###return(int)granularity.between(EPOCH,EPOCH.plusDays(days));###date###EPOCH.plusDays(days)###58:5:58:67
243889e7e54830d4b687126ea88a03a5d63d2783###/apache_iceberg/parquet/src/main/java/com/netflix/iceberg/parquet/ParquetIterable.java###/ParquetIterable.java###com.netflix.iceberg.parquet.ParquetIterable###iterator:###returnnewParquetIterator<>(builder.build());###reader###builder.build()###39:7:39:53
2be228d11acf4a502471e56c14594e0aed77529c###/apache_iceberg/nessie/src/main/java/org/apache/iceberg/nessie/NessieTableOperations.java###/NessieTableOperations.java###org.apache.iceberg.nessie.NessieTableOperations###doCommit:TableMetadata TableMetadata ###IcebergTablenewTable=ImmutableIcebergTable.builder().metadataLocation(newMetadataLocation).build();###newTable###ImmutableIcebergTable.builder()###105:7:105:109
0a6b1051657442991ebeca2a00f5b4f19e252c7c###/apache_iceberg/api/src/main/java/org/apache/iceberg/expressions/Evaluator.java###/Evaluator.java###org.apache.iceberg.expressions.Evaluator.EvalVisitor###startsWith:Bound<T> Literal<T> ###return((String)valueExpr.eval(struct)).startsWith((String)lit.value());###evalRes###valueExpr.eval(struct)###152:7:152:81
65319e911235a8cfed6f616955f1f92192d8a52f###/apache_iceberg/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergOutputCommitter.java###/TestHiveIcebergOutputCommitter.java###org.apache.iceberg.mr.hive.TestHiveIcebergOutputCommitter###writeRecords:String int int boolean boolean JobConf OutputCommitter ###OutputFileFactoryoutputFileFactory=newOutputFileFactory(spec,FileFormat.PARQUET,location,io,encryption,taskId.getTaskID().getId(),attemptNum,QUERY_ID+"-"+JOB_ID);###partitionId###taskId.getTaskID().getId()###277:7:279:52
5ed15d061d07874c8d11bcbad269e4b67e26d83d###/apache_iceberg/mr/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergUUIDObjectInspector.java###/TestIcebergUUIDObjectInspector.java###org.apache.iceberg.mr.hive.serde.objectinspector.TestIcebergUUIDObjectInspector###testIcebergUUIDObjectInspector:###Stringuuid=UUID.randomUUID().toString();###uuid###UUID.randomUUID()###50:5:50:48
c0e5ec6943de0835f119d1d06ea732709328f1cd###/apache_iceberg/api/src/main/java/org/apache/iceberg/PartitionSpec.java###/PartitionSpec.java###org.apache.iceberg.PartitionSpec.Builder###identity:String String ###fields.add(newPartitionField(sourceColumn.fieldId(),nextFieldId(),targetName,Transforms.identity(sourceColumn.type())));###field###newPartitionField(sourceColumn.fieldId(),nextFieldId(),targetName,Transforms.identity(sourceColumn.type()))###378:7:379:105
131c9c0eebf55d02ab88dc62677e46f2d9501048###/apache_iceberg/orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java###/OrcIterable.java###org.apache.iceberg.orc.OrcIterable###iterator:###TypeDescriptionreadOrcSchema=ORCSchemaUtil.buildOrcProjection(schema,orcFileReader.getSchema());###fileSchema###orcFileReader.getSchema()###76:5:76:105
131c9c0eebf55d02ab88dc62677e46f2d9501048###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java###/BatchDataReader.java###org.apache.iceberg.spark.source.BatchDataReader###open:FileScanTask ###iter=ORC.read(location).project(schemaWithoutConstants).split(task.start(),task.length()).createBatchedReaderFunc(fileSchema->VectorizedSparkOrcReaders.buildReader(expectedSchema,fileSchema,idToConstant)).recordsPerBatch(batchSize).filter(task.residual()).caseSensitive(caseSensitive).build();###builder###ORC.read(location).project(schemaWithoutConstants).split(task.start(),task.length()).createBatchedReaderFunc(fileSchema->VectorizedSparkOrcReaders.buildReader(expectedSchema,fileSchema,idToConstant)).recordsPerBatch(batchSize).filter(task.residual()).caseSensitive(caseSensitive)###108:7:116:20
f258bfa1fa89239f0dae18ed49f911602954de1b###/apache_iceberg/spark/src/main/java/com/netflix/iceberg/spark/SparkExpressions.java###/SparkExpressions.java###com.netflix.iceberg.spark.SparkExpressions###convert:Expression ###returnor(convert(orExpr.left()),convert(orExpr.right()));###orRight###convert(orExpr.right())###132:11:132:70
20fa2c48d312c56843efaa8cf51d35ca61ecf79a###/apache_iceberg/orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java###/OrcIterable.java###org.apache.iceberg.orc.OrcIterable###iterator:###returnnewOrcIterator(newOrcIterator(file,TypeConversion.toOrc(schema,newColumnIdMap()),start,length,newFileReader(file,config)),readerFunction.apply(schema));###orcFileReader###newFileReader(file,config)###62:5:65:39
0f2a164e6ac711c2f2900c91d86c7ab338ac64ee###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java###/HiveIcebergStorageHandler.java###org.apache.iceberg.mr.hive.HiveIcebergStorageHandler###put:Configuration Table ###config.set(InputFormatConfig.FILE_IO,SerializationUtil.serializeToBase64(table.io()));###base64Io###SerializationUtil.serializeToBase64(table.io())###209:5:209:92
5a3cd22e775dfa8bf79deab675390aad48ba79a5###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java###/BatchDataReader.java###org.apache.iceberg.spark.source.BatchDataReader###open:FileScanTask ###iter=Parquet.read(location).project(expectedSchema).split(task.start(),task.length()).createBatchedReaderFunc(fileSchema->VectorizedSparkParquetReaders.buildReader(expectedSchema,fileSchema,NullCheckingForGet.NULL_CHECKING_ENABLED)).recordsPerBatch(batchSize).filter(task.residual()).caseSensitive(caseSensitive).reuseContainers().build();###builder###Parquet.read(location).project(expectedSchema).split(task.start(),task.length()).createBatchedReaderFunc(fileSchema->VectorizedSparkParquetReaders.buildReader(expectedSchema,fileSchema,NullCheckingForGet.NULL_CHECKING_ENABLED)).recordsPerBatch(batchSize).filter(task.residual()).caseSensitive(caseSensitive).reuseContainers()###57:7:69:20
e9335c5d1696245e9671425b739ee212c6411629###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java###/IcebergInputFormat.java###org.apache.iceberg.mr.mapreduce.IcebergInputFormat.IcebergRecordReader###newParquetIterable:InputFile FileScanTask Schema ###returnapplyResidualFiltering(parquetReadBuilder.build(),task.residual(),readSchema);###parquetIterator###parquetReadBuilder.build()###382:7:382:94
7ecf6724a9a01462729e667b47e623b625a6caae###/apache_iceberg/data/src/main/java/org/apache/iceberg/data/parquet/BaseParquetReaders.java###/BaseParquetReaders.java###org.apache.iceberg.data.parquet.BaseParquetReaders.FallbackReadBuilder###struct:Types.StructType GroupType List<ParquetValueReader<?>> ###newFields.add(ParquetValueReaders.option(fieldType,fieldD,fieldReaders.get(i)));###fieldReader###fieldReaders.get(i)###99:9:99:91
f258bfa1fa89239f0dae18ed49f911602954de1b###/apache_iceberg/spark/src/main/java/com/netflix/iceberg/spark/SparkExpressions.java###/SparkExpressions.java###com.netflix.iceberg.spark.SparkExpressions###convert:Expression ###returnor(convert(orExpr.left()),convert(orExpr.right()));###orLeft###convert(orExpr.left())###132:11:132:70
5f6fc3be3400cfae858a938213ba8516acc983a5###/apache_iceberg/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java###/TestSparkReadProjection.java###org.apache.iceberg.spark.source.TestSparkReadProjection###convert:Schema Row ###Recordrecord=newRecord(schema);###schemaToConvert###schema###225:5:225:40
42c1e63333c340210752e879820eedd307f227d4###/apache_iceberg/core/src/main/java/org/apache/iceberg/SchemaUpdate.java###/SchemaUpdate.java###org.apache.iceberg.SchemaUpdate.ApplyChanges###map:Types.MapType Type Type ###TypevalueResult=field(map.fields().get(1),vResult);###valueField###map.fields().get(1)###376:7:376:62
5e3f9198e5675a852df4f0e1c28b4e3cf6630f86###/apache_iceberg/aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java###/S3OutputStream.java###org.apache.iceberg.aws.s3.S3OutputStream###write:byte[] int int ###stream.write(b,off,len);###relativeOffset###off###87:5:87:31
8058ec12da13b88a1724bbc493649f7b8342414e###/apache_iceberg/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetDefinitionLevelReader.java###/VectorizedParquetDefinitionLevelReader.java###org.apache.iceberg.arrow.vectorized.parquet.VectorizedParquetDefinitionLevelReader###readBatchOfDictionaryIds:IntVector int int NullabilityHolder VectorizedDictionaryEncodedParquetValuesReader ###setNulls(nullabilityHolder,idx,numValues,vector.getValidityBuffer());###validityBuffer###vector.getValidityBuffer()###66:13:66:85
4b8e2f4752d46971b78a45db0e5d4b4be0775f26###/apache_iceberg/core/src/test/java/org/apache/iceberg/rest/responses/TestGetNamespaceResponse.java###/TestGetNamespaceResponse.java###org.apache.iceberg.rest.responses.TestGetNamespaceResponse###deserialize:String ###returnmapper().readValue(json,GetNamespaceResponse.class).validate();###resp###mapper().readValue(json,GetNamespaceResponse.class)###160:5:160:76
0342f23a37d25c4e80c0cc0d82acae2c673e062d###/apache_iceberg/spark/src/test/java/com/netflix/iceberg/spark/source/TestDataFrameWrites.java###/TestDataFrameWrites.java###com.netflix.iceberg.spark.source.TestDataFrameWrites###writeAndValidate:Schema ###df.write().format("iceberg").mode("append").save(location.toString());###writer###df.write().format("iceberg").mode("append")###106:5:109:36
6ff9539ab3cbce988f09ac29ef131db864859904###/apache_iceberg/spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java###/SparkTestBase.java###org.apache.iceberg.spark.SparkTestBase###assertEquals:String Object[] Object[] ###Assert.assertEquals(context+"contentsshouldmatch",expectedValue,actualRow[col]);###actualValue###actualRow[col]###153:9:153:96
18b0ee76e1d694ee1a436d18ba992a8b4335ff48###/apache_iceberg/core/src/main/java/org/apache/iceberg/CachingCatalog.java###/CachingCatalog.java###org.apache.iceberg.CachingCatalog###loadTable:TableIdentifier ###returntableCache.get(canonicalizeIdentifier(ident),catalog::loadTable);###canonicalized###canonicalizeIdentifier(ident)###66:5:66:78
71d0657738fa6bdce4df0ead075afba77d93e619###/apache_iceberg/core/src/main/java/com/netflix/iceberg/avro/GenericAvroReader.java###/GenericAvroReader.java###com.netflix.iceberg.avro.GenericAvroReader.ReadBuilder###primitive:Schema ###ValueReaders.longs().read(decoder)*1000L###longs###ValueReaders.longs()###138:51:138:93
0f9370b5d0346826296e3cbb6ada591760fd4cf5###/apache_iceberg/api/src/main/java/com/netflix/iceberg/types/ReassignIds.java###/ReassignIds.java###com.netflix.iceberg.types.ReassignIds###map:Types.MapType Supplier<Type> Supplier<Type> ###returnTypes.MapType.ofOptional(sourceKeyId,sourceValueId,keyTypeFuture.get(),valueTypeFuture.get());###valueType###valueTypeFuture.get()###113:9:114:57
44c1d00b37f0d7fc2e978baad4f7a861a8335cf0###/apache_iceberg/spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java###/Reader.java###org.apache.iceberg.spark.source.Reader###Reader:Table Broadcast<FileIO> Broadcast<EncryptionManager> boolean DataSourceOptions ###Stringscheme="no_exist";###fsscheme###"no_exist"###134:7:134:34
b9634c9511c8a028074e8e5bdc54f0db47058668###/apache_iceberg/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java###/IcebergFilesCommitter.java###org.apache.iceberg.flink.sink.IcebergFilesCommitter###endInput:###dataFilesPerCheckpoint.put(Long.MAX_VALUE,ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));###currentCheckpointId###Long.MAX_VALUE###241:5:241:100
17b5ca54560a99f5c8832a844896b64ff3d505ab###/apache_iceberg/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java###/TestSparkParquetReader.java###org.apache.iceberg.spark.data.TestSparkParquetReader###testInt96TimestampProducedBySparkIsReadCorrectly:###List<InternalRow>readRows=rowsFromFile(Files.localInput(outputFilePath),schema);###parquetInputFile###Files.localInput(outputFilePath)###123:5:123:89
54f9a0ffaa0cc69a25818fcdfbc9b8bfc579fe67###/apache_iceberg/core/src/main/java/com/netflix/iceberg/ScanSummary.java###/ScanSummary.java###com.netflix.iceberg.ScanSummary.Builder###build:###try(CloseableIterable<ManifestEntry>entries=newManifestGroup(ops,table.currentSnapshot().manifests()).filterData(rowFilter).ignoreDeleted().select(SCAN_SUMMARY_COLUMNS).entries())###manifests###table.currentSnapshot().manifests()###157:7:181:8
c3dc9824b381e5e479e356be5e0f4fcf61a9fc37###/apache_iceberg/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java###/MergingSnapshotProducer.java###org.apache.iceberg.MergingSnapshotProducer###add:ManifestFile ###ManifestFilemanifestFile=ManifestWriter.copyAppendManifest(reader,manifestPath(manifestCount.getAndIncrement()),snapshotId(),appendedManifestsSummary);###newManifestPath###manifestPath(manifestCount.getAndIncrement())###209:7:210:106
ba54f9cfe37b28d3ad675bdc50daee75cda1f3c1###/apache_iceberg/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java###/MergingSnapshotProducer.java###org.apache.iceberg.MergingSnapshotProducer###updateEvent:###longsequenceNumber=ops.refresh().snapshot(snapshotId).sequenceNumber();###justSaved###ops.refresh().snapshot(snapshotId)###393:5:393:79
131c9c0eebf55d02ab88dc62677e46f2d9501048###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java###/RowDataReader.java###org.apache.iceberg.spark.source.RowDataReader###newOrcIterable:InputFile FileScanTask Schema Map<Integer,?> ###returnORC.read(location).project(readSchemaWithoutConstantAndMetadataFields).split(task.start(),task.length()).createReaderFunc(readOrcSchema->newSparkOrcReader(readSchema,readOrcSchema,idToConstant)).filter(task.residual()).caseSensitive(caseSensitive).build();###builder###ORC.read(location).project(readSchemaWithoutConstantAndMetadataFields).split(task.start(),task.length()).createReaderFunc(readOrcSchema->newSparkOrcReader(readSchema,readOrcSchema,idToConstant)).filter(task.residual()).caseSensitive(caseSensitive)###177:5:183:18
d8cc2a29364e57df95c4e50f4079bacd35e4a047###/apache_iceberg/core/src/main/java/org/apache/iceberg/SchemaUpdate.java###/SchemaUpdate.java###org.apache.iceberg.SchemaUpdate###internalAddColumn:String String boolean Type String ###Preconditions.checkArgument(schema.findField(parent+"."+name)==null,"Cannotaddcolumn,namealreadyexists:%s.%s",parent,name);###currentField###schema.findField(parent+"."+name)###141:7:142:74
9a0d154b0ba5e6d10d79e30470295c91c89c1e09###/apache_iceberg/spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java###/SparkCatalog.java###org.apache.iceberg.spark.SparkCatalog###initialize:String CaseInsensitiveStringMap ###this.tables=newHadoopTables(SparkSession.active().sessionState().newHadoopConf());###sparkSession###SparkSession.active()###390:5:390:90
71d0657738fa6bdce4df0ead075afba77d93e619###/apache_iceberg/spark/src/main/java/com/netflix/iceberg/spark/data/SparkValueReaders.java###/SparkValueReaders.java###com.netflix.iceberg.spark.data.SparkValueReaders.StringReader###read:Decoder ###Utf8string=decoder.readString(null);###utf8###null###74:7:74:46
ad1e634a4806de2d420d18e7c0e82b74409cfe69###/apache_iceberg/core/src/main/java/org/apache/iceberg/DataFilesTable.java###/DataFilesTable.java###org.apache.iceberg.DataFilesTable.FilesTableScan###planFiles:TableOperations Snapshot Expression boolean boolean boolean ###ExpressionpartitionFilter=Projections.inclusive(transformSpec(fileSchema,table().spec(),PARTITION_FIELD_PREFIX),caseSensitive).project(rowFilter);###spec###transformSpec(fileSchema,table().spec(),PARTITION_FIELD_PREFIX)###112:7:116:31
c2957eb45cc4ee0756d11ff9a9567d3f1cca0ba4###/apache_iceberg/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java###/ParquetUtil.java###org.apache.iceberg.parquet.ParquetUtil###footerMetrics:ParquetMetadata int ###updateMax(upperBounds,fieldId,field.type(),fromParquetPrimitive(field.type(),column.getPrimitiveType(),stats.genericGetMax()),statsTruncateLength);###max###fromParquetPrimitive(field.type(),column.getPrimitiveType(),stats.genericGetMax())###107:13:109:70
13002b534555fbf1b43ad824e7757293e8b49cf4###/apache_iceberg/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java###/HadoopTableOperations.java###org.apache.iceberg.hadoop.HadoopTableOperations###refresh:###this.currentMetadata=TableMetadataParser.read(this,io().newInputFile(metadataFile.toString()));###newMetadata###TableMetadataParser.read(this,io().newInputFile(metadataFile.toString()))###97:5:98:53
7cf96f096edeceac13916676c273954fdd435513###/apache_iceberg/api/src/main/java/org/apache/iceberg/util/StructProjection.java###/StructProjection.java###org.apache.iceberg.util.StructProjection###get:int Class<T> ###returnstruct.get(positionMap[pos],javaClass);###structPos###positionMap[pos]###141:5:141:52
d6cbca0c2fb3d10083b304bcbdf16bf51e3b586b###/apache_iceberg/core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesCommitManager.java###/RewriteDataFilesCommitManager.java###org.apache.iceberg.actions.RewriteDataFilesCommitManager###commitFileGroups:Set<RewriteFileGroup> ###RewriteFilesrewrite=table.newRewrite().validateFromSnapshot(startingSnapshotId).rewriteFiles(rewrittenDataFiles,addedDataFiles);###rewrite###table.newRewrite().validateFromSnapshot(startingSnapshotId)###75:5:77:59
12bf61dace38d7468d80a6d8e71a3cc42ffdd363###/apache_iceberg/spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java###/SparkScanBuilder.java###org.apache.iceberg.spark.source.SparkScanBuilder###build:###returnnewSparkBatchQueryScan(spark,table,readConf,schemaWithMetadataColumns(),filterExpressions);###expectedSchema###schemaWithMetadataColumns()###156:5:157:81
30711e2a744aa4e154c4e4c55a51fca621126363###/apache_iceberg/core/src/main/java/org/apache/iceberg/FastAppend.java###/FastAppend.java###org.apache.iceberg.FastAppend###copyManifest:ManifestFile ###returnManifestFiles.copyAppendManifest(ops.current().formatVersion(),reader,newManifestPath,snapshotId(),summaryBuilder);###current###ops.current()###112:7:113:97
a3d5c0cdc15617e55f1478e15bd6494540c3a42d###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java###/Catalogs.java###org.apache.iceberg.mr.Catalogs###loadCatalog:Configuration ###returnOptional.of(loader.load(conf));###catalog###loader.load(conf)###86:7:86:45
704cd8c2e4631798dfa913bec5c06bc4231cfd26###/apache_iceberg/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java###/ParquetUtil.java###org.apache.iceberg.parquet.ParquetUtil###updateMax:Map<Integer,Literal<?>> int Type Literal<T> MetricsMode ###upperBounds.put(id,BinaryUtil.truncateBinaryMax((Literal<ByteBuffer>)max,truncateLength));###truncatedMaxBinary###BinaryUtil.truncateBinaryMax((Literal<ByteBuffer>)max,truncateLength)###293:13:293:106
339866d72784a3e90625037d3a9de4fdafc5fcef###/apache_iceberg/data/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java###/GenericAppenderHelper.java###org.apache.iceberg.data.GenericAppenderHelper###appendToLocalFile:Table File FileFormat StructLike List<Record> ###FileAppender<Record>appender=newGenericAppenderFactory(table.schema()).newAppender(Files.localOutput(file),format);###appenderFactory###newGenericAppenderFactory(table.schema())###82:5:83:42
0c3dc7096d8ff1c95897229c38d2cdefe25285ec###/apache_iceberg/core/src/main/java/org/apache/iceberg/BaseRewriteManifests.java###/BaseRewriteManifests.java###org.apache.iceberg.BaseRewriteManifests.WriterWrapper###newWriter:###returnnewManifestWriter(spec,manifestPath(manifestSuffix.getAndIncrement()),snapshotId());###outputFile###manifestPath(manifestSuffix.getAndIncrement())###328:7:328:101
daf062053a87b2333bf2c0d2cd3c3676ab264989###/apache_iceberg/api/src/main/java/org/apache/iceberg/expressions/Projections.java###/Projections.java###org.apache.iceberg.expressions.Projections.StrictProjection###predicate:BoundPredicate<T> ###result=Expressions.or(result,((Transform<T,?>)part.transform()).projectStrict(part.name(),pred));###strictProjection###((Transform<T,?>)part.transform()).projectStrict(part.name(),pred)###254:9:256:84
9d9bab1f27c2c0b22d0024766bd062011f0817a4###/apache_iceberg/aws/src/main/java/org/apache/iceberg/aws/glue/GlueTableOperations.java###/GlueTableOperations.java###org.apache.iceberg.aws.glue.GlueTableOperations###persistGlueTable:Table Map<String,String> TableMetadata ###glue.updateTable(UpdateTableRequest.builder().catalogId(awsProperties.glueCatalogId()).databaseName(databaseName).skipArchive(awsProperties.glueCatalogSkipArchive()).tableInput(TableInput.builder().applyMutation(builder->IcebergToGlueConverter.setTableInputInformation(builder,metadata)).name(tableName).tableType(GLUE_EXTERNAL_TABLE_TYPE).parameters(parameters).build()).build());###updateTableRequest###UpdateTableRequest.builder().catalogId(awsProperties.glueCatalogId()).databaseName(databaseName).skipArchive(awsProperties.glueCatalogSkipArchive()).tableInput(TableInput.builder().applyMutation(builder->IcebergToGlueConverter.setTableInputInformation(builder,metadata)).name(tableName).tableType(GLUE_EXTERNAL_TABLE_TYPE).parameters(parameters).build())###188:7:198:21
30711e2a744aa4e154c4e4c55a51fca621126363###/apache_iceberg/core/src/main/java/org/apache/iceberg/BaseRewriteManifests.java###/BaseRewriteManifests.java###org.apache.iceberg.BaseRewriteManifests###copyManifest:ManifestFile ###returnManifestFiles.copyManifest(ops.current().formatVersion(),reader,newFile,snapshotId(),summaryBuilder,ALLOWED_ENTRY_STATUSES);###current###ops.current()###159:7:160:113
bdd7fa470434d484ba5c8eb27a73055976be2804###/apache_iceberg/spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceNestedDataBenchmark.java###/IcebergSourceNestedDataBenchmark.java###org.apache.iceberg.spark.source.IcebergSourceNestedDataBenchmark###initTable:###returntables.create(schema,partitionSpec,Maps.newHashMap(),newTableLocation());###properties###Maps.newHashMap()###55:5:55:88
42c1e63333c340210752e879820eedd307f227d4###/apache_iceberg/core/src/main/java/org/apache/iceberg/SchemaUpdate.java###/SchemaUpdate.java###org.apache.iceberg.SchemaUpdate.ApplyChanges###list:Types.ListType Type ###TypeelementResult=field(list.fields().get(0),result);###elementField###list.fields().get(0)###345:7:345:64
01d1462756db20a14a9ac67166e5bf56966861b4###/apache_iceberg/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java###/TestIcebergSourceTablesBase.java###org.apache.iceberg.spark.source.TestIcebergSourceTablesBase###testFilesTableWithSnapshotIdInheritance:###expected.add((GenericData.Record)record.get("data_file"));###file###(GenericData.Record)record.get("data_file")###274:13:274:72
5f6fc3be3400cfae858a938213ba8516acc983a5###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/data/SparkAvroWriter.java###/SparkAvroWriter.java###org.apache.iceberg.spark.data.SparkAvroWriter.WriteBuilder###map:Schema ValueWriter<?> ###returnSparkValueWriters.map(SparkValueWriters.strings(),convert(keyType),valueReader,convert(valueType));###writer###SparkValueWriters.strings()###110:7:111:91
5d781f19ad0bfdba2523c8483d51bf3cf31c7d81###/apache_iceberg/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java###/MergingSnapshotUpdate.java###org.apache.iceberg.MergingSnapshotUpdate###apply:TableMetadata ###for(manifest:mergeGroup(entry.getKey(),entry.getValue()))###manifestGroup###entry.getValue()###251:9:253:10
cafa2bb9ced3b473d1e77c23ba49f2db0a710095###/apache_iceberg/spark/src/main/java/com/netflix/iceberg/spark/source/Reader.java###/Reader.java###com.netflix.iceberg.spark.source.Reader.ScanTask###createDataReader:###returnnewIteratorReader(Iterators.transform(avro.iterator(),APPLY_PROJECTION.bind(UnsafeProjection.create(lazyType()))::invoke));###iter###avro.iterator()###229:11:230:84
a81d11e78fb447f68b2a3e8f98cf02b0c57c7f58###/apache_iceberg/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java###/TestRewriteManifests.java###org.apache.iceberg.TestRewriteManifests###testInvalidUsage:###ManifestFileinvalidDeletedFileManifest=writeManifest("manifest-file-3.avro",manifestEntry(ManifestEntry.Status.DELETED,snapshot.snapshotId(),FILE_A));###deleteEntry###manifestEntry(ManifestEntry.Status.DELETED,snapshot.snapshotId(),FILE_A)###1004:5:1006:85
6048e5a794242cb83871e3838d0d40aa71e36a91###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java###/IcebergSource.java###org.apache.iceberg.spark.source.IcebergSource###createWriter:String StructType SaveMode DataSourceOptions ###returnOptional.of(newWriter(table,options,mode==SaveMode.Overwrite,appId,wapId,dsSchema));###replacePartitions###mode==SaveMode.Overwrite###98:5:98:104
60645579c4c4e6c8de89758ee22034667c4a8e48###/apache_iceberg/core/src/main/java/com/netflix/iceberg/avro/BuildAvroProjection.java###/BuildAvroProjection.java###com.netflix.iceberg.avro.BuildAvroProjection###record:Schema List<String> Iterable<Schema.Field> ###updatedFields.add(newSchema.Field(field.name(),toOption(convert(field.type())),null,JsonProperties.NULL_VALUE));###newField###newSchema.Field(field.name(),toOption(convert(field.type())),null,JsonProperties.NULL_VALUE)###105:9:106:94
6ac23e03735d4514480a9f0155200faf7179f21b###/apache_iceberg/core/src/main/java/org/apache/iceberg/FastAppend.java###/FastAppend.java###org.apache.iceberg.FastAppend###apply:TableMetadata ###newManifests.add(writeManifest());###manifest###writeManifest()###77:7:77:41
54f9a0ffaa0cc69a25818fcdfbc9b8bfc579fe67###/apache_iceberg/core/src/main/java/com/netflix/iceberg/ManifestGroup.java###/ManifestGroup.java###com.netflix.iceberg.ManifestGroup###entries:###Iterable<Iterable<ManifestEntry>>readers=Iterables.transform(manifests,manifest->{ManifestReaderreader=ManifestReader.read(ops.newInputFile(manifest));FilteredManifestfiltered=reader.filterRows(dataFilter).select(columns);toClose.add(reader);returnIterables.filter(ignoreDeleted?filtered.liveEntries():filtered.allEntries(),entry->evaluator.eval((GenericDataFile)entry.file()));});###matchingManifests###manifests###97:5:106:12
db8248c16e99c435ff7eed8fa86bc3913af2756a###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java###/HiveIcebergStorageHandler.java###org.apache.iceberg.mr.hive.HiveIcebergStorageHandler###configureJobConf:TableDesc JobConf ###Preconditions.checkArgument(!tableDesc.getTableName().contains(TABLE_NAME_SEPARATOR),"Cannothandletable"+tableDesc.getTableName()+".Itsnamecontains'"+TABLE_NAME_SEPARATOR+"'");###tableName###tableDesc.getTableName()###116:7:117:118
486f5e449e2d5ca2fce534b0d95882e7c31c075f###/apache_iceberg/core/src/main/java/com/netflix/iceberg/BaseReplaceFiles.java###/BaseReplaceFiles.java###com.netflix.iceberg.BaseReplaceFiles###apply:TableMetadata ###deletedFiles.add(manifestEntry.file().path().toString());###deletedPath###manifestEntry.file().path().toString()###77:17:77:74
c3dc9824b381e5e479e356be5e0f4fcf61a9fc37###/apache_iceberg/core/src/main/java/org/apache/iceberg/DataFilesTable.java###/DataFilesTable.java###org.apache.iceberg.DataFilesTable.FilesTableScan###planFiles:TableOperations Snapshot Expression boolean boolean ###newManifestReadTask(ops.io(),newBaseFileScanTask(DataFiles.fromManifest(manifest),schemaString,specString,ResidualEvaluator.unpartitioned(rowFilter)),fileSchema)###residuals###ResidualEvaluator.unpartitioned(rowFilter)###128:11:130:26
3990daae506822dc6b85c7b6d8461be28ca5362e###/apache_iceberg/parquet/src/main/java/org/apache/iceberg/parquet/MessageTypeToType.java###/MessageTypeToType.java###org.apache.iceberg.parquet.MessageTypeToType###struct:GroupType List<Type> ###fields.add(optional(fieldId,field.getName(),fieldTypes.get(i)));###fieldType###fieldTypes.get(i)###82:9:82:75
69c06cead6e5ddfe58c17630c4863eb98e04ff4e###/apache_iceberg/core/src/main/java/org/apache/iceberg/types/FixupTypes.java###/FixupTypes.java###org.apache.iceberg.types.FixupTypes###field:Types.NestedField Supplier<Type> ###this.sourceType=sourceStruct.field(field.fieldId()).type();###sourceField###sourceStruct.field(field.fieldId())###86:5:86:66
282b6f9f1cae8d4fd5ff7c73de513ca91f01fddc###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java###/IcebergInputFormat.java###org.apache.iceberg.mr.mapreduce.IcebergInputFormat.IcebergRecordReader###newOrcIterable:InputFile FileScanTask Schema ###returnapplyResidualFiltering(orcReadBuilder.build(),task.residual(),readSchema);###orcIterator###orcReadBuilder.build()###364:7:364:90
e0ff71dfa8ffb54941fcc82ae127ab06f271331c###/apache_iceberg/spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/source/SparkMergeScan.java###/SparkMergeScan.java###org.apache.iceberg.spark.source.SparkMergeScan###filterFiles:Set<String> ###files=files().stream().filter(file->filteredLocations.contains(file.file().path().toString())).collect(Collectors.toList());###originalFile###files()###100:5:102:39
4000069ce50c525159c362b13355eb9887b30f2a###/apache_iceberg/core/src/main/java/com/netflix/iceberg/StreamingDelete.java###/StreamingDelete.java###com.netflix.iceberg.StreamingDelete###filterManifest:String ###if(deletePaths.contains(wrapper.set(file.path()))||inclusive.eval(file.partition()))###fileDelete###deletePaths.contains(wrapper.set(file.path()))###130:9:142:10
6ebe71c2d8869015429fb43909be6012ef9c19a3###/apache_iceberg/data/src/main/java/org/apache/iceberg/data/TableMigrationUtil.java###/TableMigrationUtil.java###org.apache.iceberg.data.TableMigrationUtil###listPartition:Map<String,String> String String PartitionSpec Configuration MetricsConfig NameMapping int ###task.executeWith(migrationService(parallelism));###service###migrationService(parallelism)###103:9:103:57
daf062053a87b2333bf2c0d2cd3c3676ab264989###/apache_iceberg/api/src/main/java/org/apache/iceberg/expressions/Projections.java###/Projections.java###org.apache.iceberg.expressions.Projections.InclusiveProjection###predicate:BoundPredicate<T> ###result=Expressions.and(result,((Transform<T,?>)part.transform()).project(part.name(),pred));###inclusiveProjection###((Transform<T,?>)part.transform()).project(part.name(),pred)###224:9:226:78
3287fee28359d303df28fef6fc9ea7b95cfa348f###/apache_iceberg/api/src/main/java/org/apache/iceberg/types/Comparators.java###/Comparators.java###org.apache.iceberg.types.Comparators.CharSeqComparator###compare:CharSequence CharSequence ###intcmp=Character.compare(s1.charAt(i),s2.charAt(i));###c1###s1.charAt(i)###191:9:191:65
649cbdde83693ebda8e8dc6e75857426d25414ec###/apache_iceberg/api/src/main/java/org/apache/iceberg/PartitionSpec.java###/PartitionSpec.java###org.apache.iceberg.PartitionSpec.Builder###month:String ###fields.add(newPartitionField(sourceColumn.fieldId(),name,Transforms.month(sourceColumn.type())));###field###newPartitionField(sourceColumn.fieldId(),name,Transforms.month(sourceColumn.type()))###347:7:348:81
6d3b1f7996d4ceee5d5e28df7f378f6fe377950e###/apache_iceberg/core/src/main/java/org/apache/iceberg/TableMetadata.java###/TableMetadata.java###org.apache.iceberg.TableMetadata###removeSnapshotLogEntries:Set<Long> ###if(!snapshotIds.contains(logEntry.snapshotId()))###snapshotId###logEntry.snapshotId()###700:7:703:8
a2796f0fd38e517d17462ebc7b0234282e976dd3###/apache_iceberg/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java###/IcebergInputFormat.java###org.apache.iceberg.mr.mapreduce.IcebergInputFormat.IcebergRecordReader###newOrcIterable:InputFile FileScanTask Schema ###GenericOrcReader.buildReader(readSchema,fileSchema,constantsMap(task,IdentityPartitionConverters::convertConstant))###idToConstant###constantsMap(task,IdentityPartitionConverters::convertConstant)###337:29:338:108
0f9370b5d0346826296e3cbb6ada591760fd4cf5###/apache_iceberg/spark/src/main/java/com/netflix/iceberg/spark/data/SparkAvroReader.java###/SparkAvroReader.java###com.netflix.iceberg.spark.data.SparkAvroReader.ReadBuilder###primitive:Schema ###ValueReaders.longs().read(decoder)*1000L###longs###ValueReaders.longs()###125:51:125:93
db8248c16e99c435ff7eed8fa86bc3913af2756a###/apache_iceberg/mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormats.java###/TestIcebergInputFormats.java###org.apache.iceberg.mr.TestIcebergInputFormats###testCustomCatalog:###conf.set("warehouse.location",temp.newFolder("hadoop_catalog").getAbsolutePath());###warehouseLocation###temp.newFolder("hadoop_catalog").getAbsolutePath()###356:5:356:88
6f23d5c7e9e36d59b1401e63fbf2e46aecf6ca90###/apache_iceberg/aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java###/S3OutputStream.java###org.apache.iceberg.aws.s3.S3OutputStream###close:###s3.putObject(PutObjectRequest.builder().bucket(location.bucket()).key(location.key()).build(),RequestBody.fromFile(stagingFile));###requestBuilder###PutObjectRequest.builder().bucket(location.bucket()).key(location.key())###94:7:96:46
01d1462756db20a14a9ac67166e5bf56966861b4###/apache_iceberg/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java###/TestIcebergSourceTablesBase.java###org.apache.iceberg.spark.source.TestIcebergSourceTablesBase###testFilesUnpartitionedTable:###expected.add((GenericData.Record)record.get("data_file"));###file###(GenericData.Record)record.get("data_file")###374:13:374:72
ad925bdf75b6c180cd367168deab47b4969c8177###/apache_iceberg/core/src/main/java/org/apache/iceberg/ManifestsTable.java###/ManifestsTable.java###org.apache.iceberg.ManifestsTable###task:TableScan ###returnStaticDataTask.of(ops.io().newInputFile(scan.snapshot().manifestListLocation()),scan.snapshot().manifests(),this::manifestFileToRow);###manifestListLocation###scan.snapshot().manifestListLocation()###82:5:85:34
466073b7d8c23ebeae045822ee6e1a1104a5ed5a###/apache_iceberg/spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/SparkFilters.java###/SparkFilters.java###org.apache.iceberg.spark.SparkFilters###convert:Filter ###Expressionchild=convert(notFilter.child());###childFilter###notFilter.child()###171:11:171:57
4b8e2f4752d46971b78a45db0e5d4b4be0775f26###/apache_iceberg/core/src/test/java/org/apache/iceberg/rest/responses/TestListNamespacesResponse.java###/TestListNamespacesResponse.java###org.apache.iceberg.rest.responses.TestListNamespacesResponse###deserialize:String ###returnmapper().readValue(json,ListNamespacesResponse.class).validate();###resp###mapper().readValue(json,ListNamespacesResponse.class)###129:5:129:78
4b8e2f4752d46971b78a45db0e5d4b4be0775f26###/apache_iceberg/core/src/test/java/org/apache/iceberg/rest/responses/TestUpdateNamespacePropertiesResponse.java###/TestUpdateNamespacePropertiesResponse.java###org.apache.iceberg.rest.responses.TestUpdateNamespacePropertiesResponse###deserialize:String ###returnmapper().readValue(json,UpdateNamespacePropertiesResponse.class).validate();###resp###mapper().readValue(json,UpdateNamespacePropertiesResponse.class)###263:5:263:89
d98224a82b104888281d4e901ccf948f9642590b###/apache_iceberg/api/src/main/java/org/apache/iceberg/types/TypeUtil.java###/TypeUtil.java###org.apache.iceberg.types.TypeUtil###indexByName:Types.StructType ###returnvisit(struct,newIndexByName());###indexer###newIndexByName()###109:5:109:45
e52b6810f7983f6cd26290a5279d6898fa904ce1###/apache_iceberg/data/src/main/java/org/apache/iceberg/data/DeleteFilter.java###/DeleteFilter.java###org.apache.iceberg.data.DeleteFilter###applyEqDeletes:###StructLikeSetdeleteSet=Deletes.toEqualitySet(CloseableIterable.transform(CloseableIterable.concat(deleteRecords),Record::copy),deleteSchema.asStruct());###records###CloseableIterable.transform(CloseableIterable.concat(deleteRecords),Record::copy)###137:7:140:36
e9a9032fc686fc9082e5db2dcafde3a72f0abfa9###/apache_iceberg/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java###/ParquetTypeVisitor.java###org.apache.iceberg.parquet.ParquetTypeVisitor###visit:Type ParquetTypeVisitor<T> ###elementResult=visitField(repeatedElement.getType(0),visitor);###elementField###repeatedElement.getType(0)###66:17:66:81
b307f40d4e89027c7bec897697afeee8943be014###/apache_iceberg/core/src/test/java/org/apache/iceberg/mapping/TestMappingUpdates.java###/TestMappingUpdates.java###org.apache.iceberg.mapping.TestMappingUpdates###testMappingUpdateFailureSkipsMappingUpdate:###table.updateProperties().set(TableProperties.DEFAULT_NAME_MAPPING,NameMappingParser.toJson(mapping)).commit();###mappingJson###NameMappingParser.toJson(mapping)###223:5:225:19
3990daae506822dc6b85c7b6d8461be28ca5362e###/apache_iceberg/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java###/SparkTableUtil.java###org.apache.iceberg.spark.SparkTableUtil###listParquetPartition:Map<String,String> String PartitionSpec Configuration MetricsConfig ###metrics=ParquetUtil.footerMetrics(ParquetFileReader.readFooter(conf,stat),metricsSpec);###metadata###ParquetFileReader.readFooter(conf,stat)###329:15:329:106
